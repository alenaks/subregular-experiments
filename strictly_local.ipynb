{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/alenaks/subregular-experiments/local_sigmapie/code\n",
      "\n",
      "You successfully loaded SigmaPie. \n",
      "\n",
      "Formal language classes and grammars available:\n",
      "\t* strictly piecewise: SP(alphabet, grammar, k, data, polar);\n",
      "\t* strictly local: SL(alphabet, grammar, k, data, edges, polar);\n",
      "\t* tier-based strictly local: TSL(alphabet, grammar, k, data, edges, polar, tier);\n",
      "\t* multiple tier-based strictly local: MTSL(alphabet, grammar, k, data, edges, polar).\n",
      "\n",
      "Alternatively, you can initialize a transducer: FST(states, sigma, gamma, initial, transitions, stout).\n",
      "Learning algorithm:\n",
      "\tOSTIA: ostia(sample, sigma, gamma).\n",
      "/home/alenaks/subregular-experiments\n"
     ]
    }
   ],
   "source": [
    "# accessing SigmaPie toolkit: I know, horrible!\n",
    "# I promise I'll make it a package soon\n",
    "\n",
    "%cd local_sigmapie/code/\n",
    "from main import *\n",
    "%cd ../.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments on SL languages\n",
    "\n",
    "## Learning toy pattern of word-final devoicing\n",
    "\n",
    "Word-final devoicing: we are trying to learn \"no voiced obstruents at the end of the word\", or `*b<` constraint. It is a pretty widespread phonological rule (German, Russian a.o.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_german = [\"aaba\", \"abp\", \"bbaaapapp\", \"pbaa\", \"pabpp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TG = SL(polar = \"n\")\n",
    "TG.alphabet = [\"a\", \"b\", \"p\"]\n",
    "TG.k = 2\n",
    "TG.data = toy_german"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TG.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('b', '<'), ('>', '<')]\n"
     ]
    }
   ],
   "source": [
    "print(TG.grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Side effect: also learns that strings cannot be empty.\n",
    "\n",
    "## Learning the word-final devoicing from the real data (German)\n",
    "\n",
    "The data comes from the [wordlist by enz](https://github.com/enz/german-wordlist).\n",
    "\n",
    "Loading the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Aa', 'Aachener', 'Aachenerin', 'Aachenerinnen', 'Aachenern', 'Aacheners', 'Aaden', 'Aak', 'Aake', 'Aaken']\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "words = []\n",
    "with codecs.open('words.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        if line != \"\":\n",
    "            words.append(line[:-1])\n",
    "            \n",
    "print(words[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the length of the wordlist?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685618\n"
     ]
    }
   ],
   "source": [
    "print(len(words))\n",
    "# total, we have 685618 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many /b/, /d/ and /g/ are final in the current dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of /b/: 1599\n",
      "Number of /d/: 15294\n",
      "Number of /g/: 17098\n"
     ]
    }
   ],
   "source": [
    "count_final_b = 0\n",
    "count_final_d = 0\n",
    "count_final_g = 0\n",
    "\n",
    "for i in words:\n",
    "    \n",
    "    if i[-1] == \"b\":\n",
    "        count_final_b += 1\n",
    "    elif i[-1] == \"d\":\n",
    "        count_final_d += 1\n",
    "    elif i[-1] == \"g\":\n",
    "        count_final_g += 1\n",
    "        \n",
    "print(\"Number of /b/:\", count_final_b) # 1599, or 0.2% words\n",
    "print(\"Number of /d/:\", count_final_d) # 15294, or 2.2% words\n",
    "print(\"Number of /g/:\", count_final_g) # 17098, or 2.4 % words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocessing, step 1: \"implementing\" word-final devoicing in the dataset\n",
    "\n",
    "In German, orthography doesn't reflect the word-final devoicing. So first of all, I rewrite all word-final /b/, /d/ and /g/ as /p/, /t/ and /k/, correspondingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_wf = []\n",
    "for w in words:\n",
    "    word = w.lower()\n",
    "    if word[-1] == \"b\":\n",
    "        word = word[:-1] + \"p\"\n",
    "    elif word[-1] == \"d\":\n",
    "        word = word[:-1] + \"t\"\n",
    "    elif word[-1] == \"g\":\n",
    "        word = word[:-1] + \"k\"\n",
    "        \n",
    "    apply_wf.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocessing, step 2\n",
    "\n",
    "Secondly, I remove words with \"non-German\" characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_wordlist = []\n",
    "banned = []\n",
    "\n",
    "ban = ['à', 'á', 'â', 'å', 'ç', 'è', 'é', 'ê', 'ë', 'í', 'î', 'ñ', 'ó', 'õ', 'ú',\n",
    "       'û', 'č', 'ē', 'ī', 'ł', 'ō', 'œ', 'š', 'ū']\n",
    "\n",
    "for w in apply_wf:\n",
    "    present = False\n",
    "    for i in ban:\n",
    "        if i in w:\n",
    "            banned.append(w)\n",
    "            present = True\n",
    "            break\n",
    "            \n",
    "    if present:\n",
    "        continue\n",
    "        \n",
    "    german_wordlist.append(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What words did we ban?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abbé', 'abbés', 'abrégé', 'abrégés', 'acheuléen', 'acheuléens', 'agrément', 'agréments', 'ampère', 'ångström', 'ångströms', 'aperçu', 'aperçus', 'apéro', 'apéros']\n"
     ]
    }
   ],
   "source": [
    "print(banned[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How long is the wordlist now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685147\n"
     ]
    }
   ],
   "source": [
    "print(len(german_wordlist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-SL model of German word-final devoicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = SL(polar = \"n\")\n",
    "G.data = german_wordlist\n",
    "G.k = 2\n",
    "G.extract_alphabet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 4.097039699554443 seconds.\n"
     ]
    }
   ],
   "source": [
    "begin = time()\n",
    "G.learn()\n",
    "end = time()\n",
    "# takes around 4-4.5 seconds on my 16 core laptop\n",
    "print(\"It took\", end - begin, \"seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did it learn the rule of the word-final devoicing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, it did!\n"
     ]
    }
   ],
   "source": [
    "if all([(goal in G.grammar) for goal in [(\"b\", \"<\"), (\"g\", \"<\"), (\"d\", \"<\")]]):\n",
    "    print(\"Yes, it did!\")\n",
    "else:\n",
    "    print(\"Nope, it didn't.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How big is the grammar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109\n"
     ]
    }
   ],
   "source": [
    "print(len(G.grammar))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's there?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('b', 'x'), ('b', 'ß'), ('b', '<'), ('c', 'j'), ('c', 'v'), ('c', 'w'), ('c', 'x'), ('c', 'ß'), ('d', 'x'), ('d', 'ß'), ('d', '<'), ('f', 'x'), ('f', 'ß'), ('g', 'x'), ('g', 'ß'), ('g', '<'), ('h', 'x'), ('h', 'ß'), ('j', 'c'), ('j', 'd')] ...\n"
     ]
    }
   ],
   "source": [
    "print(G.grammar[:20], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from the target grammar, it also learned all the bigrams that happened to not occurre in that German corpus. The next question is then **can SL grammars generate words that look German?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating fake German words\n",
    "\n",
    "I'll be using positive SL grammars instead of the negative. It's much faster since all grammars are positive by default, and switching the polarity of the grammar is $O(|\\Sigma|^k)$ operation. Okay for $k=2$, but takes ages for $k=4$ are more.\n",
    "\n",
    "### 2-SL generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['uiriäknclzndkküfsewrzbfäßmui', 'q', 'ücoxwaopmhmldctfzädwwpwpäknohämawasfyzuoh', 'ep', 'kua', 'aßszrqikqanovzhqalrntüdühuxhanbäßupmböwsödaxemouq', 'yoevdafjiffeßnjö']\n"
     ]
    }
   ],
   "source": [
    "G.switch_polarity()\n",
    "print(G.generate_sample(n = 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Absolutely horrible.\n",
    "\n",
    "### 3-SL generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['eguslugfähaesäbbyfoh', 'uvöliraoglyrrh', 'nisjeaulh', 'örm', 'tjoye']\n"
     ]
    }
   ],
   "source": [
    "G3 = SL()\n",
    "G3.data = german_wordlist\n",
    "G3.k = 3\n",
    "G3.extract_alphabet()\n",
    "G3.learn()\n",
    "print(G3.generate_sample(n = 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tiny bit better.\n",
    "\n",
    "### 4-SL generation\n",
    "\n",
    "Careful, this is incredibly slow, we can at least decrease the number of the examples we are generating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wanzyklorbnic', 'avoya', 'wyando']\n"
     ]
    }
   ],
   "source": [
    "G4 = SL()\n",
    "G4.data = german_wordlist\n",
    "G4.k = 4\n",
    "G4.extract_alphabet()\n",
    "G4.learn()\n",
    "print(G4.generate_sample(n = 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starts reminding German, but only sometimes.\n",
    "\n",
    "## What SL languages cannot learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to learn a simple harmony. The transparent element is `x`, and within one word there can be either only `a`s or `o`s, i.e. we're trying to fake vowel harmony in rounding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['axxxxaaxxa', 'ooxxooxxoo', 'xxxxaaxxxa', 'aaxxxaxxxx', 'xxxooxxxoo']\n"
     ]
    }
   ],
   "source": [
    "sl_harmony = ['axxxxaaxxa', 'ooxxooxxoo', 'xxxxaaxxxa', 'aaxxxaxxxx', 'xxxooxxxoo']\n",
    "print(sl_harmony)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 'o'), ('o', 'a'), ('>', '<')]\n"
     ]
    }
   ],
   "source": [
    "slh = SL(polar = \"n\")\n",
    "slh.data = sl_harmony\n",
    "slh.extract_alphabet()\n",
    "slh.learn()\n",
    "print(slh.grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grammar looks reasonable! Let's see what words it predicts to be grammatical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['o', 'ooxxaaa', 'aaxo']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slh.generate_sample(n = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nope, it learned those \\*ao and \\*oa generalization _locally_ but not in a _long-distant_ manner. Tier-based strictly local or strictly piecewise grammars would be more suitable here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
