{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for Chapter 3: Learning Languages\n",
    "\n",
    "This notebook provides the code written for and used in the Chapter 3 of my dissertation **_SigmaPie_ for subregular and subsequential grammar induction**. All the links will be added soon. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generators and evaluators: the setup for the experiments\n",
    "\n",
    "## Step 1: loading dependencies, including _SigmaPie_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "from random import choice, randint\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/alenaks/subregular-experiments/local_sigmapie/code\n",
      "\n",
      "You successfully loaded SigmaPie. \n",
      "\n",
      "Formal language classes and grammars available:\n",
      "\t* strictly piecewise: SP(alphabet, grammar, k, data, polar);\n",
      "\t* strictly local: SL(alphabet, grammar, k, data, edges, polar);\n",
      "\t* tier-based strictly local: TSL(alphabet, grammar, k, data, edges, polar, tier);\n",
      "\t* multiple tier-based strictly local: MTSL(alphabet, grammar, k, data, edges, polar).\n",
      "\n",
      "Alternatively, you can initialize a transducer: FST(states, sigma, gamma, initial, transitions, stout).\n",
      "Learning algorithm:\n",
      "\tOSTIA: ostia(sample, sigma, gamma).\n",
      "/home/alenaks/subregular-experiments\n"
     ]
    }
   ],
   "source": [
    "# accessing SigmaPie toolkit: I know, horrible!\n",
    "# I promise I'll make it a package soon\n",
    "%cd local_sigmapie/code/\n",
    "from main import *\n",
    "%cd ../.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: defining general harmonic evaluator\n",
    "\n",
    "Here, I will talk about the artificial harmonic generator that I will be using throughout Chapters 3 and 4 of my dissertation.\n",
    "It can generate two types of samples:\n",
    "\n",
    "* Samples of **well-formed words**, i.e. words that don't violate the rules of the harmony; and\n",
    "* Samples of **underlying -> surface forms**, i.e. pairs where the first member has only the first value of every harmonic class specified (i.e. the feature that needs to be spread is given), and all consecutive members of the same class are masked as the name of that class.\n",
    "\n",
    "### Parameters of the generator\n",
    "\n",
    "List of the parameters that are available:\n",
    "\n",
    "* number of strings to be generated;\n",
    "* harmonic classes and their members (harmonic class is a class of segments that don't co-occur unless there is a blocker in-between them);\n",
    "* minimal and maximal cluster length of each of the harmonic classes;\n",
    "* blockers and the new domain that they introduce;\n",
    "* a probability of observing a blocker (1 / n, where n is a parameter): basically means \"every n-th cluster will be the blocker\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Harmony(object):\n",
    "    \"\"\"\n",
    "    Class defining the toy generator for the harmonic datasets.\n",
    "    \n",
    "    Attributes:\n",
    "        cl_members (dict): dictionary of the type {(harmonic_class_1):class_id_1,\n",
    "            (harmonic_class_2):class_id_2, ...} that contains info about the present\n",
    "            harmonic classes. Note that the transparent element can be encoded by \n",
    "            a harmonic class containing a single element.\n",
    "            Example: {(\"a\", \"o\"):\"A\", (\"b\", \"p\"):\"B\", (\"c\"):\"C\"}\n",
    "        cl_lengths (dict): dictionary of the type {class_id:(min_len, max_len)},\n",
    "            where min_len and max_len denote the min and max len of the cluster\n",
    "            made out of elements of class_id.\n",
    "            Example: {\"A\":(1, 3), \"B\":(2, 4), \"C\":(4, 8)}\n",
    "        blockers (dict): dictionary of the type {\"b_1\":\"u_1\", \"b_2\":\"u_2\", ...} where\n",
    "            \"b\" is the blocker, and \"u\" is the newly introduced value.\n",
    "            Example: {\"t\":\"p\"}\n",
    "        blocker_prob (int): a chance of observing a blocker, the P evaluates from\n",
    "            (1/blocker_prob).\n",
    "            Example: 5\n",
    "    \"\"\"\n",
    "    def __init__(self, cl_members, cl_lengths = None, blockers = None, blocker_prob = 5):\n",
    "        \"\"\"\n",
    "        Init function for the Harmony class.\n",
    "        \"\"\"\n",
    "        self.cl_members = cl_members\n",
    "        if cl_lengths is not None:\n",
    "            self.cl_lengths = cl_lengths\n",
    "        else:\n",
    "            self.cl_lengths = {i:(1, 3) for i in self.cl_members.values()}\n",
    "        self.blockers = blockers\n",
    "        self.blocker_prob = blocker_prob\n",
    "        \n",
    "\n",
    "        \n",
    "    def generate_words(self, n = 3, length = 10):\n",
    "        \"\"\"\n",
    "        Generates n strings of a given length.\n",
    "        \n",
    "        Arguments:\n",
    "            n (int): how many strings need to be generated;\n",
    "            length (int): length of the strings.\n",
    "            \n",
    "        Returns:\n",
    "            list[str]: n generated strings.\n",
    "        \"\"\"\n",
    "        # check if the harmony rules are well-formed\n",
    "        if not self._verify_classes():\n",
    "            raise(\"Cannot generate dataset: the sets are overlapping.\")\n",
    "            \n",
    "        # unpack the dictionary for a quicker lookup\n",
    "        unpacked = self._unpack_classes()\n",
    "        transparent = self._transparent()\n",
    "        generated = [self._generate(unpacked, length) for i in range(n)]\n",
    "        return generated\n",
    "    \n",
    "\n",
    "    def generate_pairs(self, n = 3, length = 10):\n",
    "        \"\"\"\n",
    "        Generates n pairs of strings of a given length.\n",
    "        \n",
    "        Arguments:\n",
    "            n (int): how many strings need to be generated;\n",
    "            length (int): length of the strings.\n",
    "            \n",
    "        Returns:\n",
    "            list[tuple[str]]: n generated pairs of strings.\n",
    "        \"\"\"\n",
    "        transparent = self._transparent()\n",
    "        outputs = self.generate_words(n, length)\n",
    "        inputs = self._mask_words(outputs, transparent)\n",
    "        return list(zip(inputs, outputs))\n",
    "        \n",
    "        \n",
    "    def _generate(self, unpacked, length):\n",
    "        \"\"\"\n",
    "        Generates a set of strings; helper function.\n",
    "        \n",
    "        Output type: list[str]\n",
    "        \"\"\"\n",
    "        \n",
    "        # initialize the specifications of this particular string\n",
    "        string = \"\"\n",
    "        specs = self._specify()\n",
    "        \n",
    "        while len(string) < length:\n",
    "            \n",
    "            \n",
    "            # check if we can now output the blocker\n",
    "            if self.blockers is not None:\n",
    "                while randint(1, self.blocker_prob) == 1:\n",
    "                    b = choice(list(self.blockers))\n",
    "                    string += b\n",
    "                    \n",
    "                    if len(string) == length:\n",
    "                        return string\n",
    "                    \n",
    "                    # rewrite the specification because of the blocker\n",
    "                    if self.blockers[b] not in specs:\n",
    "                        for spec in specs:\n",
    "                            if unpacked[spec] == unpacked[self.blockers[b]]:\n",
    "                                specs.remove(spec)\n",
    "                                specs.append(self.blockers[b])\n",
    "                                break\n",
    "                                \n",
    "            # make sure that we don't generate cluster of the same\n",
    "            # harminic set as the previous one\n",
    "            if len(string) > 0:\n",
    "                change = string[-1] in unpacked\n",
    "            else:\n",
    "                change = False\n",
    "            \n",
    "            # select and add new possible character as many times as\n",
    "            # cl_lengths indicate\n",
    "            if not change:\n",
    "                newchar = choice(specs)\n",
    "            else:\n",
    "                collection = [i for i in specs]\n",
    "                collection.remove(string[-1])\n",
    "                newchar = choice(collection)\n",
    "            freq_b, freq_e = self.cl_lengths[unpacked[newchar]]\n",
    "            string += newchar * randint(freq_b, freq_e)\n",
    "            \n",
    "            # output\n",
    "            if len(string) > length:\n",
    "                string = \"\"\n",
    "            elif len(string) == length:\n",
    "                return string\n",
    "            \n",
    "            \n",
    "    def _mask(self, string, transparent):\n",
    "        \"\"\"\n",
    "        Masks all non-initial mentions of the specified allophone: helper function.\n",
    "        \n",
    "        Output type: str\n",
    "        \"\"\"\n",
    "        classes = {i:False for i in self.cl_members.keys()}\n",
    "        undergoers = self._undergoers()\n",
    "        new = \"\"\n",
    "        for s in string:\n",
    "            if (s in undergoers) and (s not in transparent.values()):\n",
    "                for c in classes:\n",
    "                    \n",
    "                    # rewrite the non-initial mention of the harmonic set member\n",
    "                    # as its harmony_class_id\n",
    "                    if s in c and not classes[c]:\n",
    "                        classes[c] = True\n",
    "                        new += s\n",
    "                    elif s in c:\n",
    "                        new += self.cl_members[c]\n",
    "            else:\n",
    "                new += s\n",
    "        return new\n",
    "\n",
    "    \n",
    "    def _mask_words(self, words, transparent):\n",
    "        \"\"\"\n",
    "        Masks every word of a given list; helper function.\n",
    "        \n",
    "        Output type: list[str]\n",
    "        \"\"\"\n",
    "        return [self._mask(w, transparent) for w in words]\n",
    "            \n",
    "            \n",
    "    def _undergoers(self):\n",
    "        \"\"\"\n",
    "        Collects all undergoers; helper function.\n",
    "        \n",
    "        Output type: list[char]\n",
    "        \"\"\"\n",
    "        items = []\n",
    "        for i in self.cl_members:\n",
    "            items.extend(list(i))\n",
    "        return items\n",
    "    \n",
    "    def _transparent(self):\n",
    "        \"\"\"\n",
    "        Checks if there are transparent items, i.e. if there is\n",
    "        a harmonic class or classes that only contain a single item.\n",
    "        \n",
    "        Output type: dict[str:str]\n",
    "        \"\"\"\n",
    "        transparent = dict()\n",
    "        for i in self.cl_members:\n",
    "            if len(i) == 1:\n",
    "                transparent[self.cl_members[i]] = i[0]\n",
    "        return transparent\n",
    "        \n",
    "        \n",
    "    def _verify_classes(self):\n",
    "        \"\"\"\n",
    "        Verifies that no set (harmonic sets or the set of blockers)\n",
    "        overlaps with each other.\n",
    "        \n",
    "        Output type: bool\n",
    "        \"\"\"\n",
    "        items = self._undergoers()\n",
    "        if self.blockers is not None:\n",
    "            block_ok = all([i not in items for i in self.blockers])\n",
    "        else:\n",
    "            block_ok = True\n",
    "        return len(items) == len(set(items)) and block_ok\n",
    "    \n",
    "    \n",
    "    def _unpack_classes(self):\n",
    "        \"\"\"\n",
    "        Creates a dictionary where every harmonizing element \n",
    "        is mapped to its harmonic class; helps to optimize \n",
    "        the lookup of this information.\n",
    "        \n",
    "        Output type: dict\n",
    "        \"\"\"\n",
    "        items = self._undergoers()\n",
    "        unpacked = {}\n",
    "        for i in items:\n",
    "            for j in self.cl_members:\n",
    "                if i in j:\n",
    "                    unpacked[i] = self.cl_members[j]\n",
    "        return unpacked\n",
    "\n",
    "    \n",
    "    def _specify(self):\n",
    "        \"\"\"\n",
    "        Randomly initialize a specification from all given\n",
    "        harmonic datasets.\n",
    "        \n",
    "        Output type: list[char]\n",
    "        \"\"\"\n",
    "        return list(map(choice, self.cl_members.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples of the data generated by AHG\n",
    "\n",
    "#### Parallel vowel and consonant harmonies\n",
    "Harmony of a class \"A\" that contains \"a\" and \"o\" and of a class \"B\" that contains \"b\" and \"p\". Linguistically, these are simultaneous and independent vowel and consonant harmonies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = {(\"a\", \"o\"):\"A\", (\"b\", \"p\"):\"B\"}\n",
    "h1 = Harmony(s1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's generate a sample of well-formed words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aapapppapp', 'boobooobbb', 'baaabaabaa', 'booobooboo', 'aabbbabbba']\n"
     ]
    }
   ],
   "source": [
    "print(h1.generate_words(n = 5, length = 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Harmony with a transparent element\n",
    "\n",
    "Transparent, or irrelevant items that only introduce the long-distance effect in the dataset can be modeled by providing an extra harmonic class with just a single item in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2 = {(\"a\", \"o\"):\"A\", (\"x\"):\"X\"}\n",
    "l2 = {\"A\":(1, 2), \"X\":(2, 4)}\n",
    "h2 = Harmony(s2, l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, us generate some well-formed words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['xxxxooxxxx', 'xxxxooxxoo', 'xxaxxxaxxx', 'aaxxxxaaxx', 'xxxaaxxxaa']\n"
     ]
    }
   ],
   "source": [
    "print(h2.generate_words(n = 5, length = 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parallel vowel and consonant harmonies with a blocking effect\n",
    "\n",
    "Harmony of a class \"A\" and of a class \"B\", where if \"t\" occurred, \"p\" cannot be observed anymore: class \"B\" changes its specification to \"p\". Namely, \"t\" is a blocker that only allows for \"p\" after itself.\n",
    "\n",
    "Additionally, clusters of the A-element consist usually from 1 to 3 elements, and clusters of the B-elements are 2 to 4 elements long. The probability of observing the blocker is $\\frac{1}{4}$ at every step of the generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = {(\"a\", \"o\"):\"A\", (\"b\", \"p\"):\"B\"}\n",
    "l3 = {\"A\":(1, 3), \"B\":(2, 4)}\n",
    "b3 = {\"t\":\"p\"}\n",
    "p3 = 4\n",
    "h3 = Harmony(s3, l3, b3, p3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first generate some well-formed words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aaapppaaat', 'opppoooppt', 'ttppptpppo', 'opppoppooo', 'tooppppooo']\n"
     ]
    }
   ],
   "source": [
    "print(h3.generate_words(n = 5, length = 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Turkish generators and evaluators\n",
    "\n",
    "The following two functions I will be using in order to verify the well-formedness of generated Turkish or fake Turkish words:\n",
    "  * `backness_harmony` takes a string as input and tells if that strings is well-formed with respect to the rules of Turkish backness harmony;\n",
    "  * `rounding_harmony` does the same thing for the rounding harmony."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backness_harmony(string):\n",
    "    \"\"\"\n",
    "    Tells if a string is well-formed according to rules\n",
    "    of Turkish backness harmony.\n",
    "    \"\"\"\n",
    "    front_class, back_class = \"Iaou\", \"ieOU\"\n",
    "    front, back = False, False\n",
    "    \n",
    "    for v in front_class + back_class:\n",
    "        if v in string:\n",
    "            front = True if v in front_class else front\n",
    "            back = True if v in back_class else back\n",
    "\n",
    "    return not (front and back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rounding_harmony(string):\n",
    "    \"\"\"\n",
    "    Tells if a string is well-formed according to rules\n",
    "    of Turkish rounding harmony.\n",
    "    \"\"\"\n",
    "    high, low, rounded = \"iIuU\", \"aeoO\", \"uUoO\"\n",
    "    \n",
    "    vowels = \"\".join([v for v in string if v in high + low])\n",
    "    if len(vowels) < 2:\n",
    "        return True\n",
    "    \n",
    "    ro = vowels[0] in rounded\n",
    "    \n",
    "    for v in vowels[1:]:\n",
    "        if v in low:\n",
    "            if v in rounded:\n",
    "                return False\n",
    "            ro = False\n",
    "        elif (ro and v not in rounded) or (not ro and v in rounded):\n",
    "            return False\n",
    "            \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backness_and_rounding(string):\n",
    "    return backness_harmony(string) and rounding_harmony(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, to generate simplified Turkish data I will be using `turkish_word` and `generate_turkish_words` that generate a single word and a dataset, correspondingly.\n",
    "\n",
    "Their parameters are:\n",
    "* `length` is a desired length of the Turkish word;\n",
    "* `cond` is a choice of \"consonant\" that will be separating the vowels;\n",
    "* `vowel_cluster` is a tuple of integers representing minimal and maximal length of the vowel cluster;\n",
    "* `cons_cluster` is a tuple of integers representing minimal and maximal length of the consonantal cluster;\n",
    "* `n` (available for `generate_turkish` only) is the number of the examples that need to be generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turkish_word(length = 10, cons = \"x\", vowel_cluster = (1, 2),\n",
    "                          cons_cluster = (0, 3)):\n",
    "    \"\"\"\n",
    "    This generator generates fake Turkish words: namely, the words in which\n",
    "    the harmonic system and rules of Turkish are preserved, but all consonants\n",
    "    were substituted by a single given consonant.\n",
    "    \n",
    "    Arguments:\n",
    "    * length (int): a length of a word that needs to be generated;\n",
    "    * cons (str): a single character (or an empty string if only vowels\n",
    "                  need to be generated), a \"choice\" of the consonant \n",
    "                  that makes this harmony long-distant;\n",
    "    * vowel_cluster (tuple[int, int]): a tuple of integers representing\n",
    "                                       minimal and maximal length of\n",
    "                                       the vowel cluster;\n",
    "    * cons_cluster (tuple[int, int]): a tuple of integers representing\n",
    "                                      minimal and maximal length of\n",
    "                                      the consonantal cluster.\n",
    "                                      \n",
    "    Returns:\n",
    "    * str: a fake Turkish harmonic word, where all consonants are masked.\n",
    "    \"\"\"\n",
    "    if length < 1:\n",
    "        raise ValueError(\"Words cannot be so short.\")\n",
    "    \n",
    "    vowels = {\n",
    "        (True, True, True):\"u\",\n",
    "        (True, True, False):\"I\",\n",
    "        (True, False, True):\"o\",\n",
    "        (True, False, False):\"a\",\n",
    "        (False, True, True):\"U\",\n",
    "        (False, True, False):\"i\",\n",
    "        (False, False, True):\"O\",\n",
    "        (False, False, False):\"e\"\n",
    "    }\n",
    "    \n",
    "    backness = choice([True, False])\n",
    "    height = choice([True, False])\n",
    "    rounding = choice([True, False])\n",
    "    \n",
    "    specs = (backness, height, rounding)\n",
    "    word = \"\"\n",
    "    \n",
    "    if choice([0, 1]):\n",
    "            word += \"x\" * randint(*cons_cluster)\n",
    "            \n",
    "    while len(word) < length:\n",
    "        vc = vowels[specs] * randint(*vowel_cluster)\n",
    "        \n",
    "        # this part is neededd to avoid the word-initial *oo clusters\n",
    "        if len(vc) > 1 and not height and rounding:\n",
    "            rounding = False\n",
    "            vc = vc[0] + vowels[(backness, height, rounding)] * (len(vc) - 1)\n",
    "            \n",
    "        word += vc\n",
    "        word += \"x\" * randint(*cons_cluster)\n",
    "        \n",
    "        height = choice([True, False])\n",
    "        rounding = False if not height else rounding\n",
    "        specs = (backness, height, rounding)\n",
    "        \n",
    "    return word[:length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_turkish_words(n = 10, length = 10, cons = \"x\",\n",
    "                           vowel_cluster = (1, 2), cons_cluster = (1, 3)):\n",
    "    \"\"\"\n",
    "    This generator generates a list of fake Turkish words.\n",
    "    \n",
    "    Arguments:\n",
    "    * n (int): a number of strings that need to be generated;\n",
    "    ... for the rest of the arguments, see generate_turkish_word.\n",
    "    \n",
    "    Outputs:\n",
    "    * list: the list containing n fake Turkish words.\n",
    "    \"\"\"\n",
    "    return [turkish_word(length, cons, vowel_cluster, cons_cluster) for i in range(n)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: other harmonic evaluators\n",
    "\n",
    "The function `harmonic_evaluator` below takes two arguments: `data` and `rule`. `data` is a list of words that need to be evaluated, and `rule` is the evaluation function for some concrete harmony. This function will be further used in order to evaluate the performance of the learners on the generated datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def harmonic_evaluator(data, rule):\n",
    "    \"\"\"\n",
    "    Evaluates the provided data with respect to a given\n",
    "    rule of harmony.\n",
    "    \n",
    "    Arguments:\n",
    "    * data (list[str]): a list of strings tht need to be evaluated;\n",
    "    * rule (function): a function that evaluates a string according\n",
    "                       to some harmony.\n",
    "                       \n",
    "    Results:\n",
    "    * Prints the report that shows if the data follows the rule.\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    for w in data:\n",
    "        correct = (correct + 1) if rule(w) else correct\n",
    "        \n",
    "    ratio = (correct / len(data))\n",
    "    print(f\"Percentage of harmonic words: {int(ratio * 100)}%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finnish\n",
    "\n",
    "Finally, `front_harmony` defines a function that tells if a given string follows a rule of Finnish vowel harmony."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def front_harmony(string):\n",
    "    \"\"\"\n",
    "    Tells if a string is well-formed according to rules\n",
    "    of Finnish backness harmony.\n",
    "    \"\"\"\n",
    "    front_class, back_class = \"AOy\", \"aou\"\n",
    "    front, back = False, False\n",
    "    \n",
    "    for v in front_class + back_class:\n",
    "        if v in string:\n",
    "            front = True if v in front_class else front\n",
    "            back = True if v in back_class else back\n",
    "\n",
    "    return not (front and back)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fake harmonies evaluators\n",
    "\n",
    "This section would need to eventually be redone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_harmony_no_blockers(string):\n",
    "    \"\"\"\n",
    "    Checks if a single [a, o] harmony is well-formed.\n",
    "    \"\"\"\n",
    "    return not(\"a\" in string and \"o\" in string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_harmony_with_blockers(string):\n",
    "    \"\"\"\n",
    "    Checks if a single [a, o] harmony with a blocker f:a is well-formed.\n",
    "    \"\"\"\n",
    "    if \"f\" in string:\n",
    "        s1 = string[:string.index(\"f\")]\n",
    "        s2 = string[string.index(\"f\") + 1:]\n",
    "        return single_harmony_no_blockers(s1) and (not \"o\" in s2)\n",
    "    else:\n",
    "        return single_harmony_no_blockers(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_harmony(string, group = [\"a\", \"o\", \"u\", \"e\"]):\n",
    "    \"\"\"\n",
    "    Tells if a string contains only one out of four\n",
    "    (vowel) classes; check that at most one class\n",
    "    of vowels occurs within one word.\n",
    "    \n",
    "    Arguments:\n",
    "    * string (str): a string that needs to be verified;\n",
    "    * group (list[char]): the harmonic class.\n",
    "    \"\"\"\n",
    "    assert len(group) == 4\n",
    "    classes = 0\n",
    "    \n",
    "    for i in group:\n",
    "        classes = (classes + 1) if i in string else classes\n",
    "        \n",
    "    return classes in [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_harmony_no_blockers(string):\n",
    "    \"\"\"\n",
    "    Checks if a double [a, o] and [b, p] harmony is well-formed.\n",
    "    \"\"\"\n",
    "    vowels = not(\"a\" in string and \"o\" in string)\n",
    "    consonants = not(\"b\" in string and \"p\" in string)\n",
    "    return vowels and consonants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_harmony_with_blockers(string):\n",
    "    \"\"\"\n",
    "    Checks if a double [a, o] and [b, p] harmony with a blocker t:p\n",
    "    is well-formed.\n",
    "    \"\"\"\n",
    "    if \"a\" in string and \"o\" in string:\n",
    "        return False\n",
    "    \n",
    "    if \"t\" in string:\n",
    "        s1 = string[:string.index(\"t\")]\n",
    "        s2 = string[string.index(\"t\") + 1:]\n",
    "        return double_harmony_no_blockers(s1) and (\"b\" not in s2)\n",
    "    else:\n",
    "        return double_harmony_no_blockers(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Word-final devoicing generators and evaluators\n",
    "\n",
    "The functions `word_final_devoicing` and `generate_wfd` imitate the process of word-final devoicing.\n",
    "The former one generates a string or a pair of strings (UR -> SF) implementing that rule, and the latter one generates dataset consisting of ones.\n",
    "\n",
    "Their arguments are the following:\n",
    "* `sigma` is a list of symbols that can be used in the words;\n",
    "* `devoice` contains two tuples, where the first tuple represents voiced obstruents, and the second one stands for their voiceless counterparts;\n",
    "* `length` is the length of the intended words;\n",
    "* if `pairs` is True, (UG, SF) pairs will be returned, if False, only the surface forms;\n",
    "* `n` (available only for `generate_wfd`) is a number of strings or pairs that need to be generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_final_devoicing(sigma = (\"a\", \"b\", \"p\"), devoice = ((\"b\"), (\"p\")),\n",
    "                         length = 10, pairs = False):\n",
    "    \"\"\"\n",
    "    This function generates either a word grammatical with respect to a rule\n",
    "    of the word final devoicing, or a fake UG -> SF pair.\n",
    "    \n",
    "    Arguments: \n",
    "    * sigma (list[str]): a list of symbols that can be used in the words;\n",
    "    * devoice (tuple[tuple, tuple]): the first tuple represents voiced\n",
    "                                     obstruents, and the second one stands\n",
    "                                     for their voiceless counterparts;\n",
    "    * length (int): a length of the intended words;\n",
    "    * pairs (bool): if True, (UG, SF) pairs will be returned, if False, only\n",
    "                    the surface forms.\n",
    "                    \n",
    "    Outputs:\n",
    "    * str/tuple: a string or a tuple of strings (depending on the parameter \n",
    "                 `pairs`) representing the application of the word-final \n",
    "                 devoicing.\n",
    "    \"\"\"\n",
    "    if length < 1:\n",
    "        raise ValueError(\"The string has a very weird length.\")\n",
    "        \n",
    "    before, after = devoice\n",
    "    string = \"\".join([choice(sigma) for i in range(length)])\n",
    "    \n",
    "    if string[-1] not in before:\n",
    "        return (string, string) if pairs else string\n",
    "    \n",
    "    devoiced = string[:-1] + after[before.index(string[-1])]\n",
    "    return (string, devoiced) if pairs else devoiced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_wfd(n = 10, sigma = (\"a\", \"b\", \"p\"), devoice = ((\"b\"), (\"p\")),\n",
    "                 length = 10, pairs = False):\n",
    "    \"\"\"\n",
    "    Generates a set of strings or pairs that satisfy the rule of\n",
    "    the word-final devoicing.\n",
    "    \n",
    "    Arguments:\n",
    "    * n (int): the number of strings that need to be generated;\n",
    "    ... for the rest of the arguments see word_final_devoicing.\n",
    "    \n",
    "    Outputs:\n",
    "    * list: a list of strings or tuples (depending on the parameter `pairs`)\n",
    "            representing the application of the word-final devoicing.\n",
    "    \"\"\"\n",
    "    return [word_final_devoicing(sigma, devoice, length, pairs) for i in range(n)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function `evaluate_wfd_words` evaluates words with respect to the rules of the word-final devoicing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_wfd_words(data, voiced = (\"b\")):\n",
    "    \"\"\"\n",
    "    Evaluates the provided words with respect to the rule \n",
    "    of the word-final devoicing.\n",
    "    \n",
    "    Arguments:\n",
    "    * data (list[str]): a list of strings tht need to be evaluated;\n",
    "    * voiced (tuple[char]): a list of voiced characters, i.e. those\n",
    "                            that cannot be word-final.\n",
    "                       \n",
    "    Results:\n",
    "    * Prints the report that shows if the data follows the ule.\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    for w in data:\n",
    "        \n",
    "        if not len(w):\n",
    "            correct += 1\n",
    "            continue\n",
    "            \n",
    "        correct = (correct + 1) if w[-1] not in voiced else correct\n",
    "        \n",
    "    ratio = (correct / len(data))\n",
    "    print(f\"Percentage of well-formed words: {int(ratio * 100)}%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we can generate some words or pairs of words representing the rule of the word-final devoicing, and then check if the evaluator considers that those datasets are well-formed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of well-formed words: 100%.\n"
     ]
    }
   ],
   "source": [
    "evaluate_wfd_words(generate_wfd(n = 1000, pairs = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: UTP generator and evalurator\n",
    "\n",
    "The function `generate_tonal_pattern` takes a length of the string that needs to be generated, and returns a random string of raising (H) and falling (L) tones as output. `utp_tones` takes that string of tones as input, and rewrites it according to the UTP rules: no L tones are allowed in-between two H tones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tonal_pattern(length = 5):\n",
    "    \"\"\" Generates a random sequence of tones of a given length. \"\"\"\n",
    "    return \"\".join(choice([\"H\", \"L\"]) for i in range(length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utp_tones(string):\n",
    "    \"\"\" Rewrites a tonal string with respect to the rules of UTP. \"\"\"\n",
    "    \n",
    "    if set(string) not in [{\"H\", \"L\"}, {\"H\"}, {\"L\"}, set(\"\")]:\n",
    "        print(string)\n",
    "        raise ValueError(\"Unexpected symbols in the tonal string!\")\n",
    "    if not (\"H\" in string and \"L\" in string):\n",
    "        return string\n",
    "    \n",
    "    first_h = string.find(\"H\")\n",
    "    last_h = len(string) - string[::-1].find(\"H\")\n",
    "    return string[:first_h] + \"H\" * (last_h - first_h) + string[last_h:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, `generate_utp_strings` generates strings of tones that are well-formed accroding to the rules of UTP. As before, `n` signifies the number of strings that need to be generated, and `length` is the length of those strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_utp_strings(n = 10, length = 5):\n",
    "    \"\"\" Generates n strings of tones that follow UTP rules. \"\"\"\n",
    "    return [utp_tones(generate_tonal_pattern(length)) for i in range(n)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, `evaluate_utp_strings` and `evaluate_utp_pairs` calculate what is the percentage of the input data (strings or pairs of strings) is well-formed with respect to the rules of UTP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_utp_strings(data):\n",
    "    \"\"\" Evaluates the correctness of if the given sample of tonal strings. \"\"\"\n",
    "    correct = 0\n",
    "    for w in data:\n",
    "        correct = (correct + 1) if utp_tones(w) == w else correct\n",
    "        \n",
    "    ratio = (correct / len(data))\n",
    "    print(f\"Percentage of well-formed tonal layers: {int(ratio * 100)}%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we can verify the correctness of the generator using the evaluation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of well-formed tonal layers: 100%.\n"
     ]
    }
   ],
   "source": [
    "evaluate_utp_strings(generate_utp_strings(n = 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: First-last harmony generators and evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_last_UR(n = 10, length = 10):\n",
    "    \"\"\" Generates URs of first-last harmony words. \"\"\"\n",
    "    strings = []\n",
    "    for i in range(n):\n",
    "        new = choice([\"a\", \"o\"])\n",
    "        new += \"\".join([choice([\"a\", \"o\", \"x\"]) for j in range(length - 2)])\n",
    "        new += choice([\"a\", \"o\"])\n",
    "        strings.append(new)\n",
    "    return strings\n",
    "\n",
    "def first_last(string):\n",
    "    \"\"\" Makes the first and the last segment of the string the same. \"\"\"\n",
    "    return string[:-1] + string[0]\n",
    "\n",
    "def first_last_words(n = 10, length = 10):\n",
    "    \"\"\" Generates N first-last words. \"\"\"\n",
    "    return [first_last(w) for w in first_last_UR(n, length)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_first_last_words(data):\n",
    "    \"\"\"\n",
    "    Evaluates the correctness of if the given sample\n",
    "    of first-last harmony (UR -> SF).\n",
    "    \"\"\"\n",
    "    newdata = [i for i in data if len(i) > 1]\n",
    "    correct = 0\n",
    "    for w in newdata:\n",
    "        if w[0] == w[-1]:\n",
    "            correct += 1\n",
    "        \n",
    "    ratio = (correct / len(newdata))\n",
    "    print(f\"Percentage of first-last harmonic words: {int(ratio * 100)}%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary functions \\[to be eliminated\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SP generator needs to be checked with an empty negative alphabet: it's incredibly slow, something is going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sp_empty_word(alphabet, length = 5):\n",
    "    return \"\".join([choice(alphabet) for i in range(length)])\n",
    "\n",
    "def generate_sp_empty(alphabet, n = 10, length = 5):\n",
    "    return [generate_sp_empty_word(alphabet, length) for i in range(n)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing training samples for the experiments\n",
    "\n",
    "### Experiment 1: Word-final devoicing\n",
    "\n",
    "#### Artificial grammar: `toy_wfd`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bapbabbppp', 'aaapppbaap', 'bppaaapbap', 'apabbpbapp', 'bpbabbbaap', 'appappbbbp', 'bpbbababbp', 'baabpbapap', 'ppbppbpbpp', 'pbppbabaap', 'aababppbbp', 'apabbpppba', 'abpbpbpaap', 'bppapaappa', 'bbabbpbpbp']\n"
     ]
    }
   ],
   "source": [
    "toy_wfd = generate_wfd(n = 1000)\n",
    "print(toy_wfd[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Raw German data: `german_wfd`\n",
    "\n",
    "In German, orthography doesn't reflect the word-final devoicing. So first of all, I rewrite all word-final /b/, /d/ and /g/ as /p/, /t/ and /k/, correspondingly. Additionally, I also remove words with \"non-German\" characters. The data comes from the [wordlist by enz](https://github.com/enz/german-wordlist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685618\n",
      "['Aa', 'Aachener', 'Aachenerin', 'Aachenerinnen', 'Aachenern', 'Aacheners', 'Aaden', 'Aak', 'Aake', 'Aaken'] ...\n"
     ]
    }
   ],
   "source": [
    "german_data = []\n",
    "with codecs.open('german.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        if line != \"\":\n",
    "            german_data.append(line[:-1])\n",
    "            \n",
    "print(len(german_data))\n",
    "print(german_data[:10], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of final /b/: 1599\n",
      "Number of final /d/: 15294\n",
      "Number of final /g/: 17098\n"
     ]
    }
   ],
   "source": [
    "count_final_b = 0\n",
    "count_final_d = 0\n",
    "count_final_g = 0\n",
    "\n",
    "for i in german_data:\n",
    "    if i[-1] == \"b\":\n",
    "        count_final_b += 1\n",
    "    elif i[-1] == \"d\":\n",
    "        count_final_d += 1\n",
    "    elif i[-1] == \"g\":\n",
    "        count_final_g += 1\n",
    "        \n",
    "print(\"Number of final /b/:\", count_final_b) # 1599, or 0.2% words\n",
    "print(\"Number of final /d/:\", count_final_d) # 15294, or 2.2% words\n",
    "print(\"Number of final /g/:\", count_final_g) # 17098, or 2.4 % words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685147\n",
      "Clean dataset: ['aa', 'aachener', 'aachenerin', 'aachenerinnen', 'aachenern', 'aacheners', 'aaden', 'aak', 'aake', 'aaken', 'aakerbeere', 'aakerbeeren', 'aakes', 'aaks', 'aal'] ...\n",
      "\n",
      "471\n",
      "Banned words: ['abbé', 'abbés', 'abrégé', 'abrégés', 'acheuléen', 'acheuléens', 'agrément', 'agréments', 'ampère', 'ångström'] ...\n"
     ]
    }
   ],
   "source": [
    "ban = ['à', 'á', 'â', 'å', 'ç', 'è', 'é', 'ê', 'ë', 'í', 'î', 'ñ', 'ó', 'õ', 'ú',\n",
    "       'û', 'č', 'ē', 'ī', 'ł', 'ō', 'œ', 'š', 'ū']\n",
    "\n",
    "german_wfd = []\n",
    "banned_words = []\n",
    "\n",
    "for w in german_data:\n",
    "    \n",
    "    word = w.lower()\n",
    "    \n",
    "    illegal = False\n",
    "    for b in ban:\n",
    "        if b in word:\n",
    "            banned_words.append(word)\n",
    "            illegal = True\n",
    "            break\n",
    "            \n",
    "    if illegal:\n",
    "        continue\n",
    "        \n",
    "    if word[-1] == \"b\":\n",
    "        word = word[:-1] + \"p\"\n",
    "    elif word[-1] == \"d\":\n",
    "        word = word[:-1] + \"t\"\n",
    "    elif word[-1] == \"g\":\n",
    "        word = word[:-1] + \"k\"\n",
    "        \n",
    "    german_wfd.append(word)\n",
    "\n",
    "print(len(german_wfd))\n",
    "print(\"Clean dataset:\", german_wfd[:15], \"...\\n\")\n",
    "\n",
    "print(len(banned_words))\n",
    "print(\"Banned words:\", banned_words[:10], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Masked German data: `german_wfd_masked`\n",
    "\n",
    "Now, let us substitute all segments that are not /p/, /t/, /k/, /b/, /d/, /g/ by \"a\".\n",
    "It will help further to try the learning algorithms on data that has less local dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685147\n",
      "Masked words: ['aakaabaaaa', 'aakaabaaaaa', 'aakaa', 'aaka', 'aaa'] ...\n"
     ]
    }
   ],
   "source": [
    "german_wfd_masked = []\n",
    "for w in german_wfd:\n",
    "    new = \"\"\n",
    "    for s in w:\n",
    "        if s in [\"p\", \"t\", \"k\", \"b\", \"d\", \"g\"]:\n",
    "            new += s\n",
    "        else:\n",
    "            new += \"a\"\n",
    "    german_wfd_masked.append(new)\n",
    "german_data.append(\"\")\n",
    "    \n",
    "print(len(german_wfd_masked))\n",
    "print(\"Masked words:\", german_wfd_masked[10:15], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2: One vowel harmony, no blockers\n",
    "\n",
    "#### Artificial grammar: `toy_vhnb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['xxaxxaxxaa', 'xxxaxxaaxx', 'axxxxaxxxx', 'xxoxxooxxo', 'xxxxaxxxxa', 'xxxxoxxxoo', 'axxxaaxxxx', 'ooxxxoxxxo', 'aaxxxaxxxa', 'aaxxaxxaxx', 'xxxooxxoxx', 'ooxxxoxxoo', 'xxxxaaxxxa', 'xxxxaxxxxa', 'ooxxxoxxxo'] ...\n"
     ]
    }
   ],
   "source": [
    "ts2 = {(\"a\", \"o\"):\"A\", (\"x\"):\"X\"}\n",
    "tl2 = {\"A\":(1, 2), \"X\":(2, 4)}\n",
    "th2 = Harmony(ts2, tl2)\n",
    "toy_vhnb = th2.generate_words(n = 1000)\n",
    "print(toy_vhnb[:15], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Raw Finnish data: `finnish_harmony`\n",
    "\n",
    "The next step is to have a dataset from a natural language that implements a single harmony.\n",
    "Here, I use Finnish data from [this link](https://github.com/douglasbuzatto/WordLists/blob/master/finnish-words.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "287699\n",
      "['/* WP Hardening - 2016-06-19 19:09:32.261648 *', 'a', 'aa', 'aaa', 'aaaaaah', 'aaah', 'aaassa', 'aab', 'aaberge', 'aabraham'] ...\n"
     ]
    }
   ],
   "source": [
    "finnish_data = []\n",
    "with codecs.open('finnish.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        if line != \"\":\n",
    "            finnish_data.append(line[:-2])\n",
    "            \n",
    "print(len(finnish_data))\n",
    "print(finnish_data[:10], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I filter the unharmonic stems and clean the data. Apart from the digits and punctuations, I also filter words that contain `}` that stands here in this dataset for Swedish `å`, and therefore is ill-defined in terms of the harmony. Then I rewrite `{` as `ä` and `|` as `ö` in order to normalize the spelling with respect to Turkish examples further. Finally, I filter non-harmonic stems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250805\n",
      "Clean dataset: ['liitettAvAA', 'liitetyksi', 'liitetyt', 'liitetAAn', 'liitingin', 'liito', 'liitoille', 'liitoilleen', 'liitoissa', 'liitoista', 'liitoistaan', 'liitoksen', 'liitoksena', 'liitokset', 'liitoksi'] ...\n",
      "\n",
      "331\n",
      "Banned words: ['bl}baer', 'bl}field', 'bl}fieldin', 'bl}sar', 'bl}sare'] ...\n",
      "\n",
      "36563\n",
      "Non-harmonic words: ['aakkosjArjestykseen', 'aakkosjArjestyksessA', 'aaltoliikettA'] ...\n"
     ]
    }
   ],
   "source": [
    "ban = [' ', '*', '-', '.', '/', '0', '1', '2', '3', '4', '6', '8', '9', ':', '}']\n",
    "\n",
    "finnish_harmony = []\n",
    "banned_words = []\n",
    "non_harmonic = []\n",
    "\n",
    "for w in finnish_data:\n",
    "    \n",
    "    word = w.lower()\n",
    "    \n",
    "    illegal = False\n",
    "    for b in ban:\n",
    "        if b in word:\n",
    "            banned_words.append(word)\n",
    "            illegal = True\n",
    "            break\n",
    "            \n",
    "    if illegal:\n",
    "        continue\n",
    "    \n",
    "    word = word.replace(\"{\", \"A\")\n",
    "    word = word.replace(\"|\", \"O\")\n",
    "    if front_harmony(word):\n",
    "        finnish_harmony.append(word)\n",
    "    else:\n",
    "        non_harmonic.append(word)\n",
    "\n",
    "print(len(finnish_harmony))\n",
    "print(\"Clean dataset:\", finnish_harmony[105000:105015], \"...\\n\")\n",
    "\n",
    "print(len(banned_words))\n",
    "print(\"Banned words:\", banned_words[10:15], \"...\\n\")\n",
    "\n",
    "print(len(non_harmonic))\n",
    "print(\"Non-harmonic words:\", non_harmonic[:3], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Masked Finnish data: `finnish_harmony_masked`\n",
    "\n",
    "Finally, I create a dataset in which I mask all the transparent Finnish elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250805\n",
      "Masked words: ['xauxaxxxxxoxxxxa', 'xauxaxxxxxoxxxxx', 'xauxaxxxxxxxax', 'xauxaxxxx', 'xauxaxxxxuxxxxx'] ...\n"
     ]
    }
   ],
   "source": [
    "finnish_harmony_masked = []\n",
    "for w in finnish_harmony:\n",
    "    new = \"\"\n",
    "    for s in w:\n",
    "        if s in [\"A\", \"O\", \"y\", \"a\", \"o\", \"u\"]:\n",
    "            new += s\n",
    "        else:\n",
    "            new += \"x\"\n",
    "    finnish_harmony_masked.append(new)\n",
    "    \n",
    "print(len(finnish_harmony_masked))\n",
    "print(\"Masked words:\", finnish_harmony_masked[170005:170010], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3: One vowel harmony with blockers\n",
    "\n",
    "#### Artificial grammar: `toy_vhwb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['xooxxxfxxa', 'fxaafaxxxf', 'oxxooxooxx', 'aaxxxaaxxx', 'xoxxoofxxa', 'xxxaxxaaxx', 'aaxxxaxxxa', 'xxfaxxxaxx', 'xaxaaxxafx', 'axxaaxxxax', 'xafxaxxxax', 'axxxaxfxxa', 'xxxfaxaafa', 'xoxxxooxoo', 'aaxxaxaaxx'] ...\n"
     ]
    }
   ],
   "source": [
    "harmonic_classes = {(\"a\", \"o\"):\"A\", (\"x\"):\"X\"}\n",
    "blockers = {\"f\":\"a\"}\n",
    "cluster_lengths = {\"A\":(1, 2), \"X\":(1, 3)}\n",
    "blocker_prob = 5\n",
    "h = Harmony(harmonic_classes, cluster_lengths, blockers, blocker_prob)\n",
    "toy_vhwb = h.generate_words(n = 1000)\n",
    "print(toy_vhwb[:15], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 4: Two vowel harmonies, no blockers\n",
    "\n",
    "#### Artificial grammar: `toy_shnb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['xxxxaxxxxa', 'xxxxuxxxuu', 'xxxuuxxuxx', 'oxxxooxxxx', 'xxxaxxxxaa', 'eexxxexxee', 'xxxxaxxxaa', 'xxxexxxexx', 'xxaxxxxaxx', 'uxxxuxxxxu', 'exxxeexxee', 'uuxxxxuxxu', 'xxuuxxxxuu', 'xxxaxxxxaa', 'xxaaxxxxaa'] ...\n"
     ]
    }
   ],
   "source": [
    "is2 = {(\"a\", \"e\", \"o\", \"u\"):\"A\", (\"x\"):\"X\"}\n",
    "il2 = {\"A\":(1, 2), \"X\":(2, 4)}\n",
    "ih2 = Harmony(is2, il2)\n",
    "toy_shnb = ih2.generate_words(n = 1000)\n",
    "print(toy_shnb[:15], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 5: Two vowel harmonies with vowel blockers\n",
    "\n",
    "#### Artificial grammar: `toy_mhwb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['uxxxaaxx', 'Oexxexxe', 'ixxeexxx', 'xIIxxaxa', 'oxaxxIII', 'eexeexee', 'uuaxxxax', 'xoaIxxax', 'IIxxaaIx', 'xxxIaxxx', 'eexxxixx', 'xxxUxxxU', 'Oexeeiie', 'xxIIxaxx', 'UUUUexxx'] ...\n"
     ]
    }
   ],
   "source": [
    "toy_mhwb = generate_turkish_words(n = 5000, length = 8, cons_cluster = (0, 3))\n",
    "toy_mhwb.extend(generate_turkish_words(n = 5000, length = 6, cons_cluster = (0, 3)))\n",
    "toy_mhwb.extend(generate_turkish_words(n = 5000, length = 4, cons_cluster = (0, 3)))\n",
    "print(toy_mhwb[:15], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Raw Turkish data: `turkish_harmony`\n",
    "\n",
    "The following is a dataset of Turkish harmony from [here](http://www.swarthmore.edu/SocSci/harmony/public_html/dummyresults.html). I remove non-native Turkish words, and also filter the ones that do not follow the rules of backness and rounding harmony."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "890\n",
      "['ey-', 'gadr-', 'eG-', 'kesr-', 'tard-', 'keyf-', 'kos-', 'ledel-', 'garb-', 'ekto-', 'ekz-', 'fasl-', 'elektrik-Gi', 'elektro-', 'terkib-', 'abs-', 'lem-', 'koyn-', 'sUlUUk-u', 'hacr-', 'hacz-', 'hadd-', 'tesb-', 'li-', 'kIral-', 'hafid-', 'kriyo-', 'kriz-', 'hakk-', 'kIKr-'] ...\n",
      "\n",
      "10545\n",
      "['kesad', 'konukomKu', 'kesafet', 'somaki', 'kesan', 'kesat', 'lagemut', 'lagos', 'fuzuli', 'eyalet', 'rufai', 'ruhulkudUs', 'gaavur', 'gaavurca', 'gabardin', 'gabari', 'gabavet', 'kesedar', 'somye', 'konvansiyon', 'kooperatif', 'koordinasyon', 'sondeyiK', 'gabi', 'gabin', 'eylUUl', 'gabro', 'eylUl', 'eytam', 'gaco'] ...\n",
      "\n",
      "14434\n",
      "['som', 'lafazan', 'konuk', 'kekti', 'lafzan', 'konukCu', 'somak', 'laGar', 'laGIm', 'konulmak', 'somruk', 'laGIv', 'konum', 'somun', 'kesb', 'somurdanmak', 'konuk', 'somurmak', 'romanyalI', 'ru', 'ey', 'fuzul', 'gaah', 'eyer', 'gaasIb', 'eyercilik', 'rum', 'eyi', 'rumca', 'eyice'] ...\n"
     ]
    }
   ],
   "source": [
    "banned = []\n",
    "non_harmonic = []\n",
    "turkish_harmony = []\n",
    "\n",
    "with codecs.open('turkish.txt', encoding='utf-8') as f:\n",
    "    \n",
    "    ban = [\"!\", \"-\", \"w\", \"x\", \"A\"]\n",
    "    for line in f:\n",
    "        if line == \"\":\n",
    "            continue\n",
    "        w = line[:-1]\n",
    "        \n",
    "        if any([(i in w) for i in ban]):\n",
    "            banned.append(w)\n",
    "            continue\n",
    "            \n",
    "        if backness_harmony(w) and rounding_harmony(w):\n",
    "            w = w.replace(\"K\", \"k\")\n",
    "            turkish_harmony.append(w)\n",
    "        else:\n",
    "            non_harmonic.append(w)\n",
    "            \n",
    "print(len(banned))\n",
    "print(banned[:30], \"...\\n\")\n",
    "\n",
    "print(len(non_harmonic))\n",
    "print(non_harmonic[:30], \"...\\n\")\n",
    "            \n",
    "print(len(turkish_harmony))\n",
    "print(turkish_harmony[:30], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Masked Turkish data: `turkish_harmony_masked`\n",
    "Then, I simplify the Turkish harmonic data by masking all non-vowels as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14434\n",
      "Masked words: ['xOxxex', 'xaxxaxxIxxax', 'xOxxUx', 'xaxxaxIx', 'xOxex'] ...\n"
     ]
    }
   ],
   "source": [
    "turkish_harmony_masked = []\n",
    "for w in turkish_harmony:\n",
    "    new = \"\"\n",
    "    for s in w:\n",
    "        if s in \"iIuUaeoO\":\n",
    "            new += s\n",
    "        else:\n",
    "            new += \"x\"\n",
    "    turkish_harmony_masked.append(new)\n",
    "    \n",
    "print(len(turkish_harmony_masked))\n",
    "print(\"Masked words:\", turkish_harmony_masked[12005:12010], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 6: Vowel harmony and consonant harmony, no blockers\n",
    "\n",
    "#### Artificial grammar: `toy_dhnb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['boobbbobbo', 'bbbaaabbba', 'pppaaapppa', 'opppopooop', 'abaabababb', 'ooopoopppo', 'bboooboboo', 'aababaaabb', 'oopppooopo', 'bbaabbaaab', 'ooobbbobbo', 'bbooobbboo', 'poppoopopo', 'bbbabbaaab', 'bbaaabbbab'] ...\n"
     ]
    }
   ],
   "source": [
    "iss = {(\"a\", \"o\"):\"A\", (\"b\", \"p\"):\"B\"}\n",
    "ihs = Harmony(iss)\n",
    "toy_dhnb = ihs.generate_words(n = 1000)\n",
    "print(toy_dhnb[:15], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 7: Vowel harmony and consonant harmony with blockers\n",
    "\n",
    "#### Artificial grammar: `toy_dhwb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['paatppappt', 'aapattapaa', 'aabaababba', 'ttpappaatt', 'pootpototo', 'optopotppt', 'bboboboboo', 'ootpoopopp', 'opooppoopt', 'bbaatpptap', 'bobtpopoot', 'ppooppoopo', 'pataatpapt', 'optppopoot', 'aabbabbaab'] ...\n"
     ]
    }
   ],
   "source": [
    "aa = {(\"a\", \"o\"):\"A\", (\"b\", \"p\"):\"B\"}\n",
    "bb = {\"A\":(1, 2), \"B\":(1, 2)}\n",
    "cc = {\"t\":\"p\"}\n",
    "dd = 5\n",
    "hmm = Harmony(aa, bb, cc, dd)\n",
    "toy_dhwb = hmm.generate_words(n = 5000)\n",
    "print(toy_dhwb[:15], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 8: Tonal plateauing\n",
    "#### Artificial grammar: `toy_utp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LLHHH', 'LLLLH', 'LHHLL', 'LHHHH', 'HHHHH', 'HHHHL', 'HHHHH', 'HHHHH', 'LLHHL', 'HHHHL', 'HHHHH', 'HHHHH', 'LHHHL', 'HHHLL', 'LHHHH'] ...\n"
     ]
    }
   ],
   "source": [
    "toy_utp = generate_utp_strings(n = 1000)\n",
    "print(toy_utp[:15], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 9: First-last harmony\n",
    "#### Artificial grammar: `first_last_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['oaxxaxxoxo', 'oooaaaaaxo', 'aooaaxxooa', 'oooxoooxoo', 'ooooxoxaxo', 'aooaxaxxaa', 'aaxaaxaxoa', 'oaoaoxoxao', 'oaaoxooxoo', 'aaaaoxaxoa', 'ooaooxoxxo', 'oxoaaxxoao', 'aaoaoxaaxa', 'oaaoxooxao', 'axxxoaaaaa'] ...\n"
     ]
    }
   ],
   "source": [
    "first_last_data = first_last_words(n = 5000)\n",
    "print(first_last_data[:15], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick reference to the datasets\n",
    "\n",
    "* **Word-final devoicing**\n",
    "  * `toy_wfd` (1,000 words)\n",
    "  * `german_wfd` (685,147 words)\n",
    "  * `german_wfd_masked` (685,147 words)\n",
    "  \n",
    "  \n",
    "* **Single vowel harmony, no blockers**\n",
    "  * `toy_vhnb` (1,000 words)\n",
    "  * `finnish_harmony` (250,805 words)\n",
    "  * `finnish_harmony_masked` (250,805 words)\n",
    "  \n",
    "  \n",
    "* **Single vowel harmony with blockers**\n",
    "  * `toy_vhwb` (1,000 words)\n",
    "  \n",
    "    \n",
    "* **Two vowel harmonies, no blockers**\n",
    "  * `toy_shnb` (1,000 words)\n",
    "  \n",
    "  \n",
    "* **Two vowel harmonies with vowel blockers**\n",
    "  * `toy_mhwb` (15,000 words)\n",
    "  * `turkish_harmony` (14,434 words)\n",
    "  * `turkish_harmony_masked` (14,434 words)\n",
    "  \n",
    "  \n",
    "* **Vowel harmony and consonant harmony, no blockers**\n",
    "  * `toy_dhnb` (1,000 words)\n",
    "  \n",
    "  \n",
    "* **Vowel harmony and consonant harmony with blockers**\n",
    "  * `toy_dhwb` (1,000 words)\n",
    "  \n",
    "  \n",
    "* **Unboundedd tonal plateauing**\n",
    "  * `toy_utp` (1,000 words)\n",
    "  \n",
    "  \n",
    "* **First-last harmony**\n",
    "  * `first_last_data` (5,000 words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strictly local experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Word-final devoicing\n",
    "\n",
    "### Artificial grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of well-formed words: 100%.\n",
      "--------------------------\n",
      "Generates such strings: ['pbbbpbbapbpbaappppaaabapa', 'pp', 'apbp', 'bba', 'a', 'p', 'bp', 'pa', 'aapaba', 'bapp', 'p', 'apapbba', 'apa', 'bp', 'aap']\n",
      "--------------------------\n",
      "Size of the grammar: 2\n",
      "--------------------------\n",
      "First 30 restrictions: [('b', '<'), ('>', '<')]\n"
     ]
    }
   ],
   "source": [
    "this = \"sl1\"\n",
    "globals()[this] = SL(polar = \"n\")\n",
    "globals()[this].data = toy_wfd\n",
    "globals()[this].extract_alphabet()\n",
    "globals()[this].learn()\n",
    "globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\n",
    "evaluate_wfd_words(globals()[this+\"_sample\"])\n",
    "print(\"--------------------------\")\n",
    "print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "print(\"--------------------------\")\n",
    "print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "print(\"--------------------------\")\n",
    "print(\"First 30 restrictions:\", globals()[this].grammar[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### German simplified word-final devoicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this = \"sl2\"\n",
    "# globals()[this] = SL(polar = \"n\")\n",
    "# globals()[this].data = german_wfd_masked\n",
    "# globals()[this].extract_alphabet()\n",
    "# globals()[this].learn()\n",
    "# globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\n",
    "# evaluate_wfd_words(globals()[this+\"_sample\"], voiced = (\"b\", \"d\", \"g\"))\n",
    "# print(\"--------------------------\")\n",
    "# print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "# print(\"--------------------------\")\n",
    "# print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "# print(\"--------------------------\")\n",
    "# print(\"First 30 restrictions:\", globals()[this].grammar[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### German word-final devoicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this = \"sl3\"\n",
    "# globals()[this] = SL(polar = \"n\")\n",
    "# globals()[this].data = german_wfd\n",
    "# globals()[this].extract_alphabet()\n",
    "# globals()[this].learn()\n",
    "# globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\n",
    "# evaluate_wfd_words(globals()[this+\"_sample\"], voiced = (\"b\", \"d\", \"g\"))\n",
    "# print(\"--------------------------\")\n",
    "# print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "# print(\"--------------------------\")\n",
    "# print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "# print(\"--------------------------\")\n",
    "# print(\"First 30 restrictions:\", globals()[this].grammar[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Single vowel harmony, no blockers\n",
    "\n",
    "### Artificial grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of harmonic words: 82%.\n",
      "--------------------------\n",
      "Generates such strings: ['o', 'o', 'ooooxoxoooxooxoooo', 'o', 'ox', 'aaxxxaaa', 'aa', 'a', 'oxxoxoxoo', 'oo', 'xxo', 'xaxxxaxooxxo', 'axxaa', 'xa', 'o']\n",
      "--------------------------\n",
      "Size of the grammar: 3\n",
      "--------------------------\n",
      "First 30 restrictions: [('a', 'o'), ('o', 'a'), ('>', '<')]\n"
     ]
    }
   ],
   "source": [
    "this = \"sl4\"\n",
    "globals()[this] = SL(polar = \"n\")\n",
    "globals()[this].data = toy_vhnb\n",
    "globals()[this].extract_alphabet()\n",
    "globals()[this].learn()\n",
    "globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\n",
    "harmonic_evaluator(globals()[this+\"_sample\"], single_harmony_no_blockers)\n",
    "print(\"--------------------------\")\n",
    "print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "print(\"--------------------------\")\n",
    "print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "print(\"--------------------------\")\n",
    "print(\"First 30 restrictions:\", globals()[this].grammar[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplified Finnish harmony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this = \"sl5\"\n",
    "# globals()[this] = SL(polar = \"n\")\n",
    "# globals()[this].data = finnish_harmony_masked\n",
    "# globals()[this].extract_alphabet()\n",
    "# globals()[this].learn()\n",
    "# globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\n",
    "# harmonic_evaluator(globals()[this+\"_sample\"], front_harmony)\n",
    "# print(\"--------------------------\")\n",
    "# print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "# print(\"--------------------------\")\n",
    "# print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "# print(\"--------------------------\")\n",
    "# print(\"First 30 restrictions:\", globals()[this].grammar[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finnish harmony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this = \"sl6\"\n",
    "# globals()[this] = SL(polar = \"n\")\n",
    "# globals()[this].data = finnish_harmony\n",
    "# globals()[this].extract_alphabet()\n",
    "# globals()[this].learn()\n",
    "# globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\n",
    "# harmonic_evaluator(globals()[this+\"_sample\"], front_harmony)\n",
    "# print(\"--------------------------\")\n",
    "# print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "# print(\"--------------------------\")\n",
    "# print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "# print(\"--------------------------\")\n",
    "# print(\"First 30 restrictions:\", globals()[this].grammar[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: Single vowel harmony with blockers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of harmonic words: 81%.\n",
      "--------------------------\n",
      "Generates such strings: ['ff', 'f', 'fxf', 'faxoxaafx', 'ff', 'axoxooxox', 'x', 'xoxaaafaaffaaaaa', 'x', 'afaa', 'xxoo', 'xoxfa', 'oxaxa', 'fafaff', 'xaaffff']\n",
      "--------------------------\n",
      "Size of the grammar: 4\n",
      "--------------------------\n",
      "First 30 restrictions: [('a', 'o'), ('f', 'o'), ('o', 'a'), ('>', '<')]\n"
     ]
    }
   ],
   "source": [
    "this = \"sl7\"\n",
    "globals()[this] = SL(polar = \"n\")\n",
    "globals()[this].data = toy_vhwb\n",
    "globals()[this].extract_alphabet()\n",
    "globals()[this].learn()\n",
    "globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\n",
    "harmonic_evaluator(globals()[this+\"_sample\"], single_harmony_with_blockers)\n",
    "print(\"--------------------------\")\n",
    "print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "print(\"--------------------------\")\n",
    "print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "print(\"--------------------------\")\n",
    "print(\"First 30 restrictions:\", globals()[this].grammar[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 4: Two vowel harmonies, no blockers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of harmonic words: 66%.\n",
      "--------------------------\n",
      "Generates such strings: ['eeee', 'uxuxx', 'ux', 'uu', 'exo', 'aa', 'exa', 'x', 'e', 'uxu', 'oo', 'u', 'xxaxaaa', 'axo', 'e']\n",
      "--------------------------\n",
      "Size of the grammar: 13\n",
      "--------------------------\n",
      "First 30 restrictions: [('a', 'e'), ('a', 'o'), ('a', 'u'), ('e', 'a'), ('e', 'o'), ('e', 'u'), ('o', 'a'), ('o', 'e'), ('o', 'u'), ('u', 'a'), ('u', 'e'), ('u', 'o'), ('>', '<')]\n"
     ]
    }
   ],
   "source": [
    "this = \"sl8\"\n",
    "globals()[this] = SL(polar = \"n\")\n",
    "globals()[this].data = toy_shnb\n",
    "globals()[this].extract_alphabet()\n",
    "globals()[this].learn()\n",
    "globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\n",
    "harmonic_evaluator(globals()[this+\"_sample\"], double_harmony)\n",
    "print(\"--------------------------\")\n",
    "print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "print(\"--------------------------\")\n",
    "print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "print(\"--------------------------\")\n",
    "print(\"First 30 restrictions:\", globals()[this].grammar[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 5: Two vowel harmonies with vowel blockers\n",
    "\n",
    "### Artificial grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of harmonic words: 63%.\n",
      "--------------------------\n",
      "Generates such strings: ['e', 'UxO', 'UUxxaIIIaI', 'x', 'axe', 'UxaxexOxieei', 'O', 'uxe', 'uxIxoxI', 'iixOUxOee', 'uaIxxix', 'OUe', 'xIIa', 'uaxiixUe', 'Uee']\n",
      "--------------------------\n",
      "Size of the grammar: 49\n",
      "--------------------------\n",
      "First 30 restrictions: [('I', 'O'), ('I', 'U'), ('I', 'e'), ('I', 'i'), ('I', 'o'), ('I', 'u'), ('O', 'I'), ('O', 'O'), ('O', 'a'), ('O', 'i'), ('O', 'o'), ('O', 'u'), ('U', 'I'), ('U', 'O'), ('U', 'a'), ('U', 'i'), ('U', 'o'), ('U', 'u'), ('a', 'O'), ('a', 'U'), ('a', 'e'), ('a', 'i'), ('a', 'o'), ('a', 'u'), ('e', 'I'), ('e', 'O'), ('e', 'U'), ('e', 'a'), ('e', 'o'), ('e', 'u')]\n"
     ]
    }
   ],
   "source": [
    "this = \"sl9\"\n",
    "globals()[this] = SL(polar = \"n\")\n",
    "globals()[this].data = toy_mhwb\n",
    "globals()[this].extract_alphabet()\n",
    "globals()[this].learn()\n",
    "globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\n",
    "harmonic_evaluator(globals()[this+\"_sample\"], backness_and_rounding)\n",
    "print(\"--------------------------\")\n",
    "print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "print(\"--------------------------\")\n",
    "print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "print(\"--------------------------\")\n",
    "print(\"First 30 restrictions:\", globals()[this].grammar[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplified Turkish harmony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this = \"sl10\"\n",
    "# globals()[this] = SL(polar = \"n\")\n",
    "# globals()[this].data = turkish_harmony_masked\n",
    "# globals()[this].extract_alphabet()\n",
    "# globals()[this].learn()\n",
    "# globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\n",
    "# harmonic_evaluator(globals()[this+\"_sample\"], backness_and_rounding)\n",
    "# print(\"--------------------------\")\n",
    "# print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "# print(\"--------------------------\")\n",
    "# print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "# print(\"--------------------------\")\n",
    "# print(\"First 30 restrictions:\", globals()[this].grammar[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turkish harmony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this = \"sl11\"\n",
    "# globals()[this] = SL(polar = \"n\")\n",
    "# globals()[this].data = turkish_harmony\n",
    "# globals()[this].extract_alphabet()\n",
    "# globals()[this].learn()\n",
    "# globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\n",
    "# harmonic_evaluator(globals()[this+\"_sample\"], backness_and_rounding)\n",
    "# print(\"--------------------------\")\n",
    "# print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "# print(\"--------------------------\")\n",
    "# print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "# print(\"--------------------------\")\n",
    "# print(\"First 30 restrictions:\", globals()[this].grammar[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 6: Vowel harmony and consonant harmony, no blockers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of harmonic words: 64%.\n",
      "--------------------------\n",
      "Generates such strings: ['opabaapopaboo', 'a', 'p', 'boppaa', 'opabbo', 'op', 'aapapo', 'obobooopaappp', 'ob', 'pob', 'p', 'opaabo', 'ba', 'poopa', 'opo']\n",
      "--------------------------\n",
      "Size of the grammar: 5\n",
      "--------------------------\n",
      "First 30 restrictions: [('a', 'o'), ('b', 'p'), ('o', 'a'), ('p', 'b'), ('>', '<')]\n"
     ]
    }
   ],
   "source": [
    "this = \"sl12\"\n",
    "globals()[this] = SL(polar = \"n\")\n",
    "globals()[this].data = toy_dhnb\n",
    "globals()[this].extract_alphabet()\n",
    "globals()[this].learn()\n",
    "globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\n",
    "harmonic_evaluator(globals()[this+\"_sample\"], double_harmony_no_blockers)\n",
    "print(\"--------------------------\")\n",
    "print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "print(\"--------------------------\")\n",
    "print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "print(\"--------------------------\")\n",
    "print(\"First 30 restrictions:\", globals()[this].grammar[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 7: Vowel harmony and consonant harmony with blockers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of harmonic words: 63%.\n",
      "--------------------------\n",
      "Generates such strings: ['p', 'a', 'bbbbboptab', 'oppooptatatttpp', 'tt', 'a', 'btpopab', 'opaaaa', 'obobtpp', 'topppatttpappobtaabobapot', 'ataabttpab', 'pootpooopott', 'ptapapoboptopap', 'optabaab', 'pob']\n",
      "--------------------------\n",
      "Size of the grammar: 6\n",
      "--------------------------\n",
      "First 30 restrictions: [('a', 'o'), ('b', 'p'), ('o', 'a'), ('p', 'b'), ('t', 'b'), ('>', '<')]\n"
     ]
    }
   ],
   "source": [
    "this = \"sl13\"\n",
    "globals()[this] = SL(polar = \"n\")\n",
    "globals()[this].data = toy_dhwb\n",
    "globals()[this].extract_alphabet()\n",
    "globals()[this].learn()\n",
    "globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\n",
    "harmonic_evaluator(globals()[this+\"_sample\"], double_harmony_with_blockers)\n",
    "print(\"--------------------------\")\n",
    "print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "print(\"--------------------------\")\n",
    "print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "print(\"--------------------------\")\n",
    "print(\"First 30 restrictions:\", globals()[this].grammar[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 8: Unbounded tonal plateauing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of well-formed tonal layers: 85%.\n",
      "--------------------------\n",
      "Generates such strings: ['LHHHLLL', 'LL', 'LH', 'LL', 'HL', 'LL', 'LL', 'LLHHH', 'HH', 'LL', 'LLL', 'LLH', 'HLLLHH', 'LLLHLLLL', 'HL']\n",
      "--------------------------\n",
      "Size of the grammar: 5\n",
      "--------------------------\n",
      "First 30 restrictions: [('H', 'L', 'H'), ('>', 'H', '<'), ('>', 'L', '<'), ('>', '>', '<'), ('>', '<', '<')]\n"
     ]
    }
   ],
   "source": [
    "this = \"sl14\"\n",
    "globals()[this] = SL(polar = \"n\", k = 3)\n",
    "globals()[this].data = toy_utp\n",
    "globals()[this].extract_alphabet()\n",
    "globals()[this].learn()\n",
    "globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\n",
    "evaluate_utp_strings(globals()[this+\"_sample\"])\n",
    "print(\"--------------------------\")\n",
    "print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "print(\"--------------------------\")\n",
    "print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "print(\"--------------------------\")\n",
    "print(\"First 30 restrictions:\", globals()[this].grammar[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 9: First-last harmony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of first-last harmonic words: 48%.\n",
      "--------------------------\n",
      "Generates such strings: ['aaxooaaxaaxaaoaxxaxo', 'oxao', 'axooaxxoxoxxoxoaa', 'oxxxoxaooaoaxxa', 'oo', 'oxxo', 'oxaaxoxaxxaoaxaxxxxa', 'oxo', 'ooxoaaaxxooaxa', 'ao', 'oxoooo', 'axxxxxaa', 'axxo', 'oa', 'ooooxoxa']\n",
      "--------------------------\n",
      "Size of the grammar: 13\n",
      "--------------------------\n",
      "First 30 restrictions: [('a', 'x', '<'), ('o', 'x', '<'), ('x', 'x', '<'), ('x', '<', '<'), ('>', 'a', '<'), ('>', 'o', '<'), ('>', 'x', 'a'), ('>', 'x', 'o'), ('>', 'x', 'x'), ('>', 'x', '<'), ('>', '>', 'x'), ('>', '>', '<'), ('>', '<', '<')]\n"
     ]
    }
   ],
   "source": [
    "this = \"sl15\"\n",
    "globals()[this] = SL(polar = \"n\", k = 3)\n",
    "globals()[this].data = first_last_data\n",
    "globals()[this].extract_alphabet()\n",
    "globals()[this].learn()\n",
    "globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\n",
    "evaluate_first_last_words(globals()[this+\"_sample\"])\n",
    "print(\"--------------------------\")\n",
    "print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "print(\"--------------------------\")\n",
    "print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "print(\"--------------------------\")\n",
    "print(\"First 30 restrictions:\", globals()[this].grammar[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strictly piecewise experiments\n",
    "\n",
    "## Experiment 1: Word-final devoicing\n",
    "\n",
    "### Artificial grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of well-formed words: 64%.\n",
      "--------------------------\n",
      "Generates such strings: ['', 'abpaabpaap', 'paaaap', 'ppapbpa', 'pabaaaa', 'bapppb', 'bbaapppapp', 'paapapp', 'bppbp', 'papppbpaba', 'bbappb', 'bppppbbpbppp', 'appppbba', 'aapppppbpbbp', 'abpaabbappb']\n",
      "--------------------------\n",
      "Size of the grammar: 0\n",
      "--------------------------\n",
      "First 30 restrictions: []\n"
     ]
    }
   ],
   "source": [
    "this = \"sp1\"\n",
    "globals()[this] = SP(polar = \"n\")\n",
    "globals()[this].data = toy_wfd\n",
    "globals()[this].extract_alphabet()\n",
    "globals()[this].learn()\n",
    "globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\n",
    "if not globals()[this].grammar:\n",
    "    evaluate_wfd_words(generate_sp_empty(globals()[this].alphabet, n = 1000))\n",
    "else:\n",
    "    evaluate_wfd_words(globals()[this+\"_sample\"])\n",
    "print(\"--------------------------\")\n",
    "print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "print(\"--------------------------\")\n",
    "print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "print(\"--------------------------\")\n",
    "print(\"First 30 restrictions:\", globals()[this].grammar[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### German simplified word-final devoicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this = \"sp2\"\n",
    "# globals()[this] = SP(polar = \"n\")\n",
    "# globals()[this].data = german_wfd_masked\n",
    "# globals()[this].extract_alphabet()\n",
    "# globals()[this].learn()\n",
    "# globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\n",
    "# if not globals()[this].grammar:\n",
    "#     evaluate_wfd_words(generate_sp_empty(globals()[this].alphabet, n = 1000), voiced = (\"b\", \"d\", \"g\"))\n",
    "# else:\n",
    "#     evaluate_wfd_words(globals()[this+\"_sample\"], voiced = (\"b\", \"d\", \"g\"))\n",
    "# print(\"--------------------------\")\n",
    "# print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "# print(\"--------------------------\")\n",
    "# print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "# print(\"--------------------------\")\n",
    "# print(\"First 30 restrictions:\", globals()[this].grammar[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### German word-final devoicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this = \"sp3\"\n",
    "# globals()[this] = SP(polar = \"n\")\n",
    "# globals()[this].data = german_wfd\n",
    "# globals()[this].extract_alphabet()\n",
    "# globals()[this].learn()\n",
    "# globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\n",
    "# if not globals()[this].grammar:\n",
    "#     evaluate_wfd_words(generate_sp_empty(globals()[this].alphabet, n = 1000), voiced = (\"b\", \"d\", \"g\"))\n",
    "# else:\n",
    "#     evaluate_wfd_words(globals()[this+\"_sample\"], voiced = (\"b\", \"d\", \"g\"))\n",
    "# print(\"--------------------------\")\n",
    "# print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "# print(\"--------------------------\")\n",
    "# print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "# print(\"--------------------------\")\n",
    "# print(\"First 30 restrictions:\", globals()[this].grammar[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Single vowel harmony, no blockers\n",
    "\n",
    "### Artificial grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of harmonic words: 100%.\n",
      "--------------------------\n",
      "Generates such strings: ['', 'aaxaaxxax', 'ooooxxxox', 'axaxaaxaxa', 'xaaaxxxax', 'oooooxoxoxx', 'aaxax', 'axaxaaaaxaaa', 'ooooxxoo', 'xaaaaaaaxx', 'oxooox', 'oxxxxoxx', 'xxxxxa', 'xaxaa', 'xaxaxaa']\n",
      "--------------------------\n",
      "Size of the grammar: 2\n",
      "--------------------------\n",
      "First 30 restrictions: [('a', 'o'), ('o', 'a')]\n"
     ]
    }
   ],
   "source": [
    "this = \"sp4\"\n",
    "globals()[this] = SP(polar = \"n\")\n",
    "globals()[this].data = toy_vhnb\n",
    "globals()[this].extract_alphabet()\n",
    "globals()[this].learn()\n",
    "globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\n",
    "if not globals()[this].grammar:\n",
    "    harmonic_evaluator(generate_sp_empty(globals()[this].alphabet, n = 1000), single_harmony_no_blockers)\n",
    "else:\n",
    "    harmonic_evaluator(globals()[this+\"_sample\"], single_harmony_no_blockers)\n",
    "print(\"--------------------------\")\n",
    "print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "print(\"--------------------------\")\n",
    "print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "print(\"--------------------------\")\n",
    "print(\"First 30 restrictions:\", globals()[this].grammar[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplified Finnish harmony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this = \"sp5\"\n",
    "# globals()[this] = SP(polar = \"n\")\n",
    "# globals()[this].data = finnish_harmony_masked\n",
    "# globals()[this].extract_alphabet()\n",
    "# globals()[this].learn()\n",
    "# globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\n",
    "# if not globals()[this].grammar:\n",
    "#     harmonic_evaluator(generate_sp_empty(globals()[this].alphabet, n = 1000), front_harmony)\n",
    "# else:\n",
    "#     harmonic_evaluator(globals()[this+\"_sample\"], front_harmony)\n",
    "# print(\"--------------------------\")\n",
    "# print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "# print(\"--------------------------\")\n",
    "# print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "# print(\"--------------------------\")\n",
    "# print(\"First 30 restrictions:\", globals()[this].grammar[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finnish harmony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this = \"sp6\"\n",
    "# globals()[this] = SP(polar = \"n\")\n",
    "# globals()[this].data = finnish_harmony\n",
    "# globals()[this].extract_alphabet()\n",
    "# globals()[this].learn()\n",
    "# globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\n",
    "# if not globals()[this].grammar:\n",
    "#     harmonic_evaluator(generate_sp_empty(globals()[this].alphabet, n = 1000), front_harmony)\n",
    "# else:\n",
    "#     harmonic_evaluator(globals()[this+\"_sample\"], front_harmony)\n",
    "# print(\"--------------------------\")\n",
    "# print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "# print(\"--------------------------\")\n",
    "# print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "# print(\"--------------------------\")\n",
    "# print(\"First 30 restrictions:\", globals()[this].grammar[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: Single vowel harmony with blockers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of harmonic words: 83%.\n",
      "--------------------------\n",
      "Generates such strings: ['', 'oxfxa', 'oofxf', 'xfaxaaxfffa', 'oxaaf', 'oxofaf', 'xxoaxxax', 'fxfxf', 'axxfxafx', 'xoooxoxxa', 'oxffxxa', 'fxxafa', 'oaafa', 'xfaf', 'xofaafax']\n",
      "--------------------------\n",
      "Size of the grammar: 2\n",
      "--------------------------\n",
      "First 30 restrictions: [('a', 'o'), ('f', 'o')]\n"
     ]
    }
   ],
   "source": [
    "this = \"sp7\"\n",
    "globals()[this] = SP(polar = \"n\")\n",
    "globals()[this].data = toy_vhwb\n",
    "globals()[this].extract_alphabet()\n",
    "globals()[this].learn()\n",
    "globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\n",
    "if not globals()[this].grammar:\n",
    "    harmonic_evaluator(generate_sp_empty(globals()[this].alphabet, n = 1000), single_harmony_with_blockers)\n",
    "else:\n",
    "    harmonic_evaluator(globals()[this+\"_sample\"], single_harmony_with_blockers)\n",
    "print(\"--------------------------\")\n",
    "print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "print(\"--------------------------\")\n",
    "print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "print(\"--------------------------\")\n",
    "print(\"First 30 restrictions:\", globals()[this].grammar[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 4: Two vowel harmonies, no blockers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of harmonic words: 100%.\n",
      "--------------------------\n",
      "Generates such strings: ['', 'uuxuu', 'uxuxuuu', 'exx', 'uuuuuxxuu', 'aaxax', 'eexxxxx', 'exexexx', 'axxxxxxaaa', 'uuxu', 'ex', 'uuxuuxuuuu', 'axxaxaaxaa', 'xaxaa', 'eexeeexxxex']\n",
      "--------------------------\n",
      "Size of the grammar: 12\n",
      "--------------------------\n",
      "First 30 restrictions: [('a', 'e'), ('a', 'o'), ('a', 'u'), ('e', 'a'), ('e', 'o'), ('e', 'u'), ('o', 'a'), ('o', 'e'), ('o', 'u'), ('u', 'a'), ('u', 'e'), ('u', 'o')]\n"
     ]
    }
   ],
   "source": [
    "this = \"sp8\"\n",
    "globals()[this] = SP(polar = \"n\")\n",
    "globals()[this].data = toy_shnb\n",
    "globals()[this].extract_alphabet()\n",
    "globals()[this].learn()\n",
    "globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\n",
    "if not globals()[this].grammar:\n",
    "    harmonic_evaluator(generate_sp_empty(globals()[this].alphabet, n = 1000), double_harmony)\n",
    "else:\n",
    "    harmonic_evaluator(globals()[this+\"_sample\"], double_harmony)\n",
    "print(\"--------------------------\")\n",
    "print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "print(\"--------------------------\")\n",
    "print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "print(\"--------------------------\")\n",
    "print(\"First 30 restrictions:\", globals()[this].grammar[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 5: Two vowel harmonies with vowel blockers\n",
    "\n",
    "### Artificial grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of harmonic words: 72%.\n",
      "--------------------------\n",
      "Generates such strings: ['', 'uaxxxaIaaII', 'oxIxaIaI', 'oxuxax', 'uIIxIxIIaIaxaI', 'exx', 'aIIaa', 'UUixee', 'iixieexeexxiex', 'eixiiixixex', 'Oieix', 'xaI', 'Oxxieee', 'iixiiie', 'uuIx']\n",
      "--------------------------\n",
      "Size of the grammar: 44\n",
      "--------------------------\n",
      "First 30 restrictions: [('I', 'O'), ('I', 'U'), ('I', 'e'), ('I', 'i'), ('I', 'o'), ('I', 'u'), ('O', 'I'), ('O', 'O'), ('O', 'a'), ('O', 'o'), ('O', 'u'), ('U', 'I'), ('U', 'O'), ('U', 'a'), ('U', 'o'), ('U', 'u'), ('a', 'O'), ('a', 'U'), ('a', 'e'), ('a', 'i'), ('a', 'o'), ('a', 'u'), ('e', 'I'), ('e', 'O'), ('e', 'U'), ('e', 'a'), ('e', 'o'), ('e', 'u'), ('i', 'I'), ('i', 'O')]\n"
     ]
    }
   ],
   "source": [
    "this = \"sp9\"\n",
    "globals()[this] = SP(polar = \"n\")\n",
    "globals()[this].data = toy_mhwb\n",
    "globals()[this].extract_alphabet()\n",
    "globals()[this].learn()\n",
    "globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\n",
    "if not globals()[this].grammar:\n",
    "    harmonic_evaluator(generate_sp_empty(globals()[this].alphabet, n = 1000), backness_and_rounding)\n",
    "else:\n",
    "    harmonic_evaluator(globals()[this+\"_sample\"], backness_and_rounding)\n",
    "print(\"--------------------------\")\n",
    "print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "print(\"--------------------------\")\n",
    "print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "print(\"--------------------------\")\n",
    "print(\"First 30 restrictions:\", globals()[this].grammar[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplified Turkish harmony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this = \"sp10\"\n",
    "# globals()[this] = SP(polar = \"n\")\n",
    "# globals()[this].data = turkish_harmony_masked\n",
    "# globals()[this].extract_alphabet()\n",
    "# globals()[this].learn()\n",
    "# globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\n",
    "# if not globals()[this].grammar:\n",
    "#     harmonic_evaluator(generate_sp_empty(globals()[this].alphabet, n = 1000), backness_and_rounding)\n",
    "# else:\n",
    "#     harmonic_evaluator(globals()[this+\"_sample\"], backness_and_rounding)\n",
    "# print(\"--------------------------\")\n",
    "# print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "# print(\"--------------------------\")\n",
    "# print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "# print(\"--------------------------\")\n",
    "# print(\"First 30 restrictions:\", globals()[this].grammar[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turkish harmony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this = \"sp11\"\n",
    "# globals()[this] = SP(polar = \"n\")\n",
    "# globals()[this].data = turkish_harmony\n",
    "# globals()[this].extract_alphabet()\n",
    "# globals()[this].learn()\n",
    "# globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\n",
    "# if not globals()[this].grammar:\n",
    "#     harmonic_evaluator(generate_sp_empty(globals()[this].alphabet, n = 1000), backness_and_rounding)\n",
    "# else:\n",
    "#     harmonic_evaluator(globals()[this+\"_sample\"], backness_and_rounding)\n",
    "# print(\"--------------------------\")\n",
    "# print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "# print(\"--------------------------\")\n",
    "# print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "# print(\"--------------------------\")\n",
    "# print(\"First 30 restrictions:\", globals()[this].grammar[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 6: Vowel harmony and consonant harmony, no blockers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of harmonic words: 100%.\n",
      "--------------------------\n",
      "Generates such strings: ['', 'baababa', 'paaaap', 'oooboooboobobo', 'pppppaapa', 'aapapaaap', 'oppppop', 'bboobb', 'paapapp', 'opopooop', 'ooboboboo', 'aapapppppppppa', 'apapaaap', 'obbob', 'papappa']\n",
      "--------------------------\n",
      "Size of the grammar: 4\n",
      "--------------------------\n",
      "First 30 restrictions: [('a', 'o'), ('b', 'p'), ('o', 'a'), ('p', 'b')]\n"
     ]
    }
   ],
   "source": [
    "this = \"sp12\"\n",
    "globals()[this] = SP(polar = \"n\")\n",
    "globals()[this].data = toy_dhnb\n",
    "globals()[this].extract_alphabet()\n",
    "globals()[this].learn()\n",
    "globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\n",
    "if not globals()[this].grammar:\n",
    "    harmonic_evaluator(generate_sp_empty(globals()[this].alphabet, n = 1000), double_harmony_no_blockers)\n",
    "else:\n",
    "    harmonic_evaluator(globals()[this+\"_sample\"], double_harmony_no_blockers)\n",
    "print(\"--------------------------\")\n",
    "print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "print(\"--------------------------\")\n",
    "print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "print(\"--------------------------\")\n",
    "print(\"First 30 restrictions:\", globals()[this].grammar[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 7: Vowel harmony and consonant harmony with blockers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of harmonic words: 83%.\n",
      "--------------------------\n",
      "Generates such strings: ['', 'bottttpto', 'totoppotp', 'pooptp', 'ottpo', 'bpaatp', 'bpopott', 'obppptot', 'potto', 'atapap', 'aptpata', 'bpttaat', 'pot', 'tto', 'bappatttapa']\n",
      "--------------------------\n",
      "Size of the grammar: 4\n",
      "--------------------------\n",
      "First 30 restrictions: [('a', 'o'), ('o', 'a'), ('p', 'b'), ('t', 'b')]\n"
     ]
    }
   ],
   "source": [
    "this = \"sp13\"\n",
    "globals()[this] = SP(polar = \"n\")\n",
    "globals()[this].data = toy_dhwb\n",
    "globals()[this].extract_alphabet()\n",
    "globals()[this].learn()\n",
    "globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\n",
    "if not globals()[this].grammar:\n",
    "    harmonic_evaluator(generate_sp_empty(globals()[this].alphabet, n = 1000), double_harmony_with_blockers)\n",
    "else:\n",
    "    harmonic_evaluator(globals()[this+\"_sample\"], double_harmony_with_blockers)\n",
    "print(\"--------------------------\")\n",
    "print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "print(\"--------------------------\")\n",
    "print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "print(\"--------------------------\")\n",
    "print(\"First 30 restrictions:\", globals()[this].grammar[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 8: Unbounded tonal plateauing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of well-formed tonal layers: 100%.\n",
      "--------------------------\n",
      "Generates such strings: ['', 'LHHHHHHHL', 'LHLL', 'LHHLLL', 'HHHLLLLLLLLL', 'HLLLLLL', 'LLLLL', 'LHLLLLLL', 'HHHLLLLL', 'LLHLLLL', 'HHHLLL', 'LLHHL', 'LLHHHL', 'LH', 'LLLHH']\n",
      "--------------------------\n",
      "Size of the grammar: 1\n",
      "--------------------------\n",
      "First 30 restrictions: [('H', 'L', 'H')]\n"
     ]
    }
   ],
   "source": [
    "this = \"sp14\"\n",
    "globals()[this] = SP(polar = \"n\", k = 3)\n",
    "globals()[this].data = toy_utp\n",
    "globals()[this].extract_alphabet()\n",
    "globals()[this].learn()\n",
    "globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 100)\n",
    "if not globals()[this].grammar:\n",
    "    evaluate_utp_strings(generate_sp_empty(globals()[this].alphabet, n = 100))\n",
    "else:\n",
    "    evaluate_utp_strings(globals()[this+\"_sample\"])\n",
    "print(\"--------------------------\")\n",
    "print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "print(\"--------------------------\")\n",
    "print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "print(\"--------------------------\")\n",
    "print(\"First 30 restrictions:\", globals()[this].grammar[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 9: First-last harmony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of first-last harmonic words: 37%.\n",
      "--------------------------\n",
      "Generates such strings: ['', 'a', 'xxoa', 'oxxaao', 'xaaoa', 'aaoo', 'xa', 'aooax', 'xaoaoaaaoaxx', 'aaa', 'ooo', 'xoxax', 'oaxaa', 'oa', 'ao']\n",
      "--------------------------\n",
      "Size of the grammar: 0\n",
      "--------------------------\n",
      "First 30 restrictions: []\n"
     ]
    }
   ],
   "source": [
    "this = \"sp15\"\n",
    "globals()[this] = SP(polar = \"n\")\n",
    "globals()[this].data = first_last_data\n",
    "globals()[this].extract_alphabet()\n",
    "globals()[this].learn()\n",
    "globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 100)\n",
    "if not globals()[this].grammar:\n",
    "    evaluate_first_last_words(generate_sp_empty(globals()[this].alphabet, n = 100))\n",
    "else:\n",
    "    evaluate_first_last_words(globals()[this+\"_sample\"])\n",
    "print(\"--------------------------\")\n",
    "print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "print(\"--------------------------\")\n",
    "print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "print(\"--------------------------\")\n",
    "print(\"First 30 restrictions:\", globals()[this].grammar[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tier-based strictly local experiments\n",
    "\n",
    "## Experiment 1: Word-final devoicing\n",
    "\n",
    "### Artificial grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of well-formed words: 100%.\n",
      "--------------------------\n",
      "Generates such strings: ['abaabbaba', 'p', 'abapabappbpbpbapa', 'ba', 'aappbbbpaabba', 'pbabbba', 'abbppa', 'pbbaabbbbpbappbpba', 'pbp', 'abpbp', 'papa', 'bbaap', 'paaappba', 'p', 'p']\n",
      "--------------------------\n",
      "Size of the grammar: 2\n",
      "--------------------------\n",
      "Tier: ['a', 'b', 'p']\n",
      "--------------------------\n",
      "First 30 restrictions: [('b', '<'), ('>', '<')]\n"
     ]
    }
   ],
   "source": [
    "this = \"tsl1\"\n",
    "globals()[this] = TSL(polar = \"n\")\n",
    "globals()[this].data = toy_wfd\n",
    "globals()[this].extract_alphabet()\n",
    "globals()[this].learn()\n",
    "globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\n",
    "evaluate_wfd_words(globals()[this+\"_sample\"])\n",
    "print(\"--------------------------\")\n",
    "print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "print(\"--------------------------\")\n",
    "print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "print(\"--------------------------\")\n",
    "print(\"Tier:\", globals()[this].tier)\n",
    "print(\"--------------------------\")\n",
    "print(\"First 30 restrictions:\", globals()[this].grammar[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### German simplified word-final devoicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this = \"tsl2\"\n",
    "# globals()[this] = TSL(polar = \"n\")\n",
    "# globals()[this].data = german_wfd_masked\n",
    "# globals()[this].extract_alphabet()\n",
    "# globals()[this].learn()\n",
    "# globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\n",
    "# evaluate_wfd_words(globals()[this+\"_sample\"], voiced = (\"b\", \"d\", \"g\"))\n",
    "# print(\"--------------------------\")\n",
    "# print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "# print(\"--------------------------\")\n",
    "# print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "# print(\"--------------------------\")\n",
    "# print(\"Tier:\", globals()[this].tier)\n",
    "# print(\"--------------------------\")\n",
    "# print(\"First 30 restrictions:\", globals()[this].grammar[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### German word-final devoicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this = \"tsl3\"\n",
    "# globals()[this] = TSL(polar = \"n\")\n",
    "# globals()[this].data = german_wfd\n",
    "# globals()[this].extract_alphabet()\n",
    "# globals()[this].learn()\n",
    "# globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\n",
    "# evaluate_wfd_words(globals()[this+\"_sample\"], voiced = (\"b\", \"d\", \"g\"))\n",
    "# print(\"--------------------------\")\n",
    "# print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "# print(\"--------------------------\")\n",
    "# print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "# print(\"--------------------------\")\n",
    "# print(\"Tier:\", globals()[this].tier)\n",
    "# print(\"--------------------------\")\n",
    "# print(\"First 30 restrictions:\", globals()[this].grammar[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Single vowel harmony, no blockers\n",
    "\n",
    "### Artificial grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of harmonic words: 100%.\n",
      "--------------------------\n",
      "Generates such strings: ['xxaxxaxxa', 'xxoxoxox', 'x', 'xxaxax', 'xxx', 'xaxax', 'xax', 'xx', 'xox', 'xxoxox', 'x', 'ax', 'xx', 'xxo', 'xxxaxx']\n",
      "--------------------------\n",
      "Size of the grammar: 2\n",
      "--------------------------\n",
      "Tier: ['a', 'o']\n",
      "--------------------------\n",
      "First 30 restrictions: [('a', 'o'), ('o', 'a')]\n"
     ]
    }
   ],
   "source": [
    "this = \"tsl4\"\n",
    "globals()[this] = TSL(polar = \"n\")\n",
    "globals()[this].data = toy_vhnb\n",
    "globals()[this].data.append(\"\") # added to eliminate *>< on all tiers\n",
    "globals()[this].extract_alphabet()\n",
    "globals()[this].learn()\n",
    "globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\n",
    "harmonic_evaluator(globals()[this+\"_sample\"], single_harmony_no_blockers)\n",
    "print(\"--------------------------\")\n",
    "print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "print(\"--------------------------\")\n",
    "print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "print(\"--------------------------\")\n",
    "print(\"Tier:\", globals()[this].tier)\n",
    "print(\"--------------------------\")\n",
    "print(\"First 30 restrictions:\", globals()[this].grammar[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplified Finnish harmony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this = \"tsl5\"\n",
    "# globals()[this] = TSL(polar = \"n\")\n",
    "# globals()[this].data = finnish_harmony_masked\n",
    "# globals()[this].data.append(\"\") # added to eliminate *>< on all tiers\n",
    "# globals()[this].extract_alphabet()\n",
    "# globals()[this].learn()\n",
    "# globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\n",
    "# harmonic_evaluator(globals()[this+\"_sample\"], front_harmony)\n",
    "# print(\"--------------------------\")\n",
    "# print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "# print(\"--------------------------\")\n",
    "# print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "# print(\"--------------------------\")\n",
    "# print(\"Tier:\", globals()[this].tier)\n",
    "# print(\"--------------------------\")\n",
    "# print(\"First 30 restrictions:\", globals()[this].grammar[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finnish harmony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this = \"tsl6\"\n",
    "# globals()[this] = TSL(polar = \"n\")\n",
    "# globals()[this].data = finnish_harmony\n",
    "# globals()[this].data.append(\"\") # added to eliminate *>< on all tiers\n",
    "# globals()[this].extract_alphabet()\n",
    "# globals()[this].learn()\n",
    "# globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\n",
    "# harmonic_evaluator(globals()[this+\"_sample\"], front_harmony)\n",
    "# print(\"--------------------------\")\n",
    "# print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "# print(\"--------------------------\")\n",
    "# print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "# print(\"--------------------------\")\n",
    "# print(\"Tier:\", globals()[this].tier)\n",
    "# print(\"--------------------------\")\n",
    "# print(\"First 30 restrictions:\", globals()[this].grammar[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: Single vowel harmony with blockers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of harmonic words: 100%.\n",
      "--------------------------\n",
      "Generates such strings: ['xx', 'oxxfx', 'xfxxfxxafxxfxx', 'x', 'xfxxfxx', 'xxx', 'xxoxx', 'faxx', 'xxx', 'xaxxaxfxxaxxaxxx', 'xxxfx', 'xaxx', 'xox', 'xxxffxaxxfxx', 'x']\n",
      "--------------------------\n",
      "Size of the grammar: 3\n",
      "--------------------------\n",
      "Tier: ['a', 'f', 'o']\n",
      "--------------------------\n",
      "First 30 restrictions: [('a', 'o'), ('f', 'o'), ('o', 'a')]\n"
     ]
    }
   ],
   "source": [
    "this = \"tsl7\"\n",
    "globals()[this] = TSL(polar = \"n\")\n",
    "globals()[this].data = toy_vhwb\n",
    "globals()[this].data.append(\"\") # added to eliminate *>< on all tiers\n",
    "globals()[this].extract_alphabet()\n",
    "globals()[this].learn()\n",
    "globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\n",
    "harmonic_evaluator(globals()[this+\"_sample\"], single_harmony_with_blockers)\n",
    "print(\"--------------------------\")\n",
    "print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "print(\"--------------------------\")\n",
    "print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "print(\"--------------------------\")\n",
    "print(\"Tier:\", globals()[this].tier)\n",
    "print(\"--------------------------\")\n",
    "print(\"First 30 restrictions:\", globals()[this].grammar[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 4: Two vowel harmonies, no blockers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of harmonic words: 100%.\n",
      "--------------------------\n",
      "Generates such strings: ['xxoxox', 'xxoxxoxx', 'xxoxxx', 'xxoxx', 'xxx', 'xxxoxxoxxooxxoxxx', 'xxxaxxax', 'xx', 'xx', 'xoxxxoxxo', 'xaxx', 'xxoxoxxoxox', 'xxxox', 'xxaaxx', 'xxoxxxoxxoxx']\n",
      "--------------------------\n",
      "Size of the grammar: 12\n",
      "--------------------------\n",
      "Tier: ['a', 'e', 'o', 'u']\n",
      "--------------------------\n",
      "First 30 restrictions: [('a', 'e'), ('a', 'o'), ('a', 'u'), ('e', 'a'), ('e', 'o'), ('e', 'u'), ('o', 'a'), ('o', 'e'), ('o', 'u'), ('u', 'a'), ('u', 'e'), ('u', 'o')]\n"
     ]
    }
   ],
   "source": [
    "this = \"tsl8\"\n",
    "globals()[this] = TSL(polar = \"n\")\n",
    "globals()[this].data = toy_shnb\n",
    "globals()[this].data.append(\"\") # added to eliminate *>< on all tiers\n",
    "globals()[this].extract_alphabet()\n",
    "globals()[this].learn()\n",
    "globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\n",
    "harmonic_evaluator(globals()[this+\"_sample\"], double_harmony)\n",
    "print(\"--------------------------\")\n",
    "print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "print(\"--------------------------\")\n",
    "print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "print(\"--------------------------\")\n",
    "print(\"Tier:\", globals()[this].tier)\n",
    "print(\"--------------------------\")\n",
    "print(\"First 30 restrictions:\", globals()[this].grammar[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 5: Two vowel harmonies with vowel blockers\n",
    "\n",
    "### Artificial grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of harmonic words: 100%.\n",
      "--------------------------\n",
      "Generates such strings: ['xxexxexxxexxixxexixixxex', 'Uxxx', 'xxxexexxixxxexxexxxixxx', 'xxuxaxxIxxxaxaxaxxaxaxxxIxxIxaxxx', 'xx', 'xx', 'xxaxxax', 'xUxxexxixx', 'xuxaxxaxxaxxIxIx', 'xxoxxuxxaxxx', 'xax', 'xxxOxexxexxxixxexxeiixxixxixxixx', 'xxaxxaxxaxxaxaxxaxxaxx', 'xxaxxxaxxxaxx', '']\n",
      "--------------------------\n",
      "Size of the grammar: 48\n",
      "--------------------------\n",
      "Tier: ['I', 'O', 'U', 'a', 'e', 'i', 'o', 'u']\n",
      "--------------------------\n",
      "First 30 restrictions: [('I', 'O'), ('I', 'U'), ('I', 'e'), ('I', 'i'), ('I', 'o'), ('I', 'u'), ('O', 'I'), ('O', 'O'), ('O', 'a'), ('O', 'i'), ('O', 'o'), ('O', 'u'), ('U', 'I'), ('U', 'O'), ('U', 'a'), ('U', 'i'), ('U', 'o'), ('U', 'u'), ('a', 'O'), ('a', 'U'), ('a', 'e'), ('a', 'i'), ('a', 'o'), ('a', 'u'), ('e', 'I'), ('e', 'O'), ('e', 'U'), ('e', 'a'), ('e', 'o'), ('e', 'u')]\n"
     ]
    }
   ],
   "source": [
    "this = \"tsl9\"\n",
    "globals()[this] = TSL(polar = \"n\")\n",
    "globals()[this].data = toy_mhwb\n",
    "globals()[this].data.append(\"\") # added to eliminate *>< on all tiers\n",
    "globals()[this].extract_alphabet()\n",
    "globals()[this].learn()\n",
    "globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\n",
    "harmonic_evaluator(globals()[this+\"_sample\"], backness_and_rounding)\n",
    "print(\"--------------------------\")\n",
    "print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "print(\"--------------------------\")\n",
    "print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "print(\"--------------------------\")\n",
    "print(\"Tier:\", globals()[this].tier)\n",
    "print(\"--------------------------\")\n",
    "print(\"First 30 restrictions:\", globals()[this].grammar[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplified Turkish harmony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this = \"tsl10\"\n",
    "# globals()[this] = TSL(polar = \"n\")\n",
    "# globals()[this].data = turkish_harmony_masked\n",
    "# globals()[this].data.append(\"\") # added to eliminate *>< on all tiers\n",
    "# globals()[this].extract_alphabet()\n",
    "# globals()[this].learn()\n",
    "# globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\n",
    "# harmonic_evaluator(globals()[this+\"_sample\"], backness_and_rounding)\n",
    "# print(\"--------------------------\")\n",
    "# print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "# print(\"--------------------------\")\n",
    "# print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "# print(\"--------------------------\")\n",
    "# print(\"Tier:\", globals()[this].tier)\n",
    "# print(\"--------------------------\")\n",
    "# print(\"First 30 restrictions:\", globals()[this].grammar[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turkish harmony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this = \"tsl11\"\n",
    "# globals()[this] = TSL(polar = \"n\")\n",
    "# globals()[this].data = turkish_harmony\n",
    "# globals()[this].data.append(\"\") # added to eliminate *>< on all tiers\n",
    "# globals()[this].extract_alphabet()\n",
    "# globals()[this].learn()\n",
    "# globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\n",
    "# harmonic_evaluator(globals()[this+\"_sample\"], backness_and_rounding)\n",
    "# print(\"--------------------------\")\n",
    "# print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "# print(\"--------------------------\")\n",
    "# print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "# print(\"--------------------------\")\n",
    "# print(\"Tier:\", globals()[this].tier)\n",
    "# print(\"--------------------------\")\n",
    "# print(\"First 30 restrictions:\", globals()[this].grammar[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 6: Vowel harmony and consonant harmony, no blockers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of harmonic words: 71%.\n",
      "--------------------------\n",
      "Generates such strings: ['', '', 'b', '', 'pppa', 'p', 'appobo', '', 'p', 'a', 'booo', 'bo', 'a', 'opaa', '']\n",
      "--------------------------\n",
      "Size of the grammar: 4\n",
      "--------------------------\n",
      "Tier: ['a', 'b', 'o', 'p']\n",
      "--------------------------\n",
      "First 30 restrictions: [('a', 'o'), ('b', 'p'), ('o', 'a'), ('p', 'b')]\n"
     ]
    }
   ],
   "source": [
    "this = \"tsl12\"\n",
    "globals()[this] = TSL(polar = \"n\")\n",
    "globals()[this].data = toy_dhnb\n",
    "globals()[this].data.append(\"\") # added to eliminate *>< on all tiers\n",
    "globals()[this].extract_alphabet()\n",
    "globals()[this].learn()\n",
    "globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\n",
    "harmonic_evaluator(globals()[this+\"_sample\"], double_harmony_no_blockers)\n",
    "print(\"--------------------------\")\n",
    "print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "print(\"--------------------------\")\n",
    "print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "print(\"--------------------------\")\n",
    "print(\"Tier:\", globals()[this].tier)\n",
    "print(\"--------------------------\")\n",
    "print(\"First 30 restrictions:\", globals()[this].grammar[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 7: Vowel harmony and consonant harmony with blockers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of harmonic words: 69%.\n",
      "--------------------------\n",
      "Generates such strings: ['aba', 'pap', 'tabtppatap', 'o', 'b', 'p', 'a', '', '', 'o', 'btttpapa', 'tab', 'oota', 'p', '']\n",
      "--------------------------\n",
      "Size of the grammar: 5\n",
      "--------------------------\n",
      "Tier: ['a', 'b', 'o', 'p', 't']\n",
      "--------------------------\n",
      "First 30 restrictions: [('a', 'o'), ('b', 'p'), ('o', 'a'), ('p', 'b'), ('t', 'b')]\n"
     ]
    }
   ],
   "source": [
    "this = \"tsl14\"\n",
    "globals()[this] = TSL(polar = \"n\")\n",
    "globals()[this].data = toy_dhwb\n",
    "globals()[this].data.append(\"\") # added to eliminate *>< on all tiers\n",
    "globals()[this].extract_alphabet()\n",
    "globals()[this].learn()\n",
    "globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\n",
    "harmonic_evaluator(globals()[this+\"_sample\"], double_harmony_with_blockers)\n",
    "print(\"--------------------------\")\n",
    "print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "print(\"--------------------------\")\n",
    "print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "print(\"--------------------------\")\n",
    "print(\"Tier:\", globals()[this].tier)\n",
    "print(\"--------------------------\")\n",
    "print(\"First 30 restrictions:\", globals()[this].grammar[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 8: First-last harmony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of first-last harmonic words: 49%.\n",
      "--------------------------\n",
      "Generates such strings: ['', '', '', 'oaoxxaaoo', 'ooo', 'oo', 'aoxxa', 'oaaxoaoxxxoxaoxxxxaa', 'aaoaxo', 'oaxa', 'oxooa', 'o', '', '', '']\n",
      "--------------------------\n",
      "Size of the grammar: 2\n",
      "--------------------------\n",
      "Tier: ['a', 'o', 'x']\n",
      "--------------------------\n",
      "First 30 restrictions: [('x', '<'), ('>', 'x')]\n"
     ]
    }
   ],
   "source": [
    "this = \"tsl15\"\n",
    "globals()[this] = TSL(polar = \"n\")\n",
    "globals()[this].data = first_last_data\n",
    "globals()[this].data.append(\"\") # added to eliminate *>< on all tiers\n",
    "globals()[this].extract_alphabet()\n",
    "globals()[this].learn()\n",
    "globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\n",
    "evaluate_first_last_words(globals()[this+\"_sample\"])\n",
    "print(\"--------------------------\")\n",
    "print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "print(\"--------------------------\")\n",
    "print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "print(\"--------------------------\")\n",
    "print(\"Tier:\", globals()[this].tier)\n",
    "print(\"--------------------------\")\n",
    "print(\"First 30 restrictions:\", globals()[this].grammar[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 9: Unbounded tonal plateauing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of well-formed tonal layers: 89%.\n",
      "--------------------------\n",
      "Generates such strings: ['HHLL', 'LLHHHL', 'LHHLL', 'HLLHLL', 'LL', 'HHLLH', 'LL', 'HL', 'HHHHH', '', 'LLHHH', 'HH', 'HHH', '', 'LLL']\n",
      "--------------------------\n",
      "Size of the grammar: 3\n",
      "--------------------------\n",
      "Tier: ['H', 'L']\n",
      "--------------------------\n",
      "First 30 restrictions: [('H', 'L', 'H'), ('>', 'H', '<'), ('>', 'L', '<')]\n"
     ]
    }
   ],
   "source": [
    "this = \"tsl13\"\n",
    "globals()[this] = TSL(polar = \"n\", k = 3)\n",
    "globals()[this].data = toy_utp\n",
    "globals()[this].data.append(\"\") # added to eliminate *>< on all tiers\n",
    "globals()[this].extract_alphabet()\n",
    "globals()[this].learn()\n",
    "globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\n",
    "evaluate_utp_strings(globals()[this+\"_sample\"])\n",
    "print(\"--------------------------\")\n",
    "print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "print(\"--------------------------\")\n",
    "print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "print(\"--------------------------\")\n",
    "print(\"Tier:\", globals()[this].tier)\n",
    "print(\"--------------------------\")\n",
    "print(\"First 30 restrictions:\", globals()[this].grammar[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple tier-based strictly local experiments\n",
    "\n",
    "## Experiment 1: Word-final devoicing\n",
    "\n",
    "### Artificial grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of well-formed words: 100%.\n",
      "--------------------------\n",
      "Generates such strings: ['aba', 'bbbpapa', '', 'a', 'pa', 'aba', 'bpa', '', 'appa', 'bpbabbp', 'app', 'bbpbap', '', 'a', 'apabp']\n",
      "--------------------------\n",
      "Size of the grammar: 1\n",
      "--------------------------\n",
      "Grammars: {('a', 'b', 'p'): [('b', '<')]}\n"
     ]
    }
   ],
   "source": [
    "this = \"mtsl1\"\n",
    "globals()[this] = MTSL(polar = \"n\")\n",
    "globals()[this].data = toy_wfd\n",
    "globals()[this].data.append(\"\") # added to eliminate *>< on all tiers\n",
    "globals()[this].extract_alphabet()\n",
    "globals()[this].learn()\n",
    "globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\n",
    "evaluate_wfd_words(globals()[this+\"_sample\"])\n",
    "print(\"--------------------------\")\n",
    "print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "print(\"--------------------------\")\n",
    "print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "print(\"--------------------------\")\n",
    "print(\"Grammars:\", globals()[this].grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### German simplified word-final devoicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this = \"mtsl2\"\n",
    "# globals()[this] = MTSL(polar = \"n\")\n",
    "# globals()[this].data = german_wfd_masked\n",
    "# globals()[this].data.append(\"\") # added to eliminate *>< on all tiers\n",
    "# globals()[this].extract_alphabet()\n",
    "# globals()[this].learn()\n",
    "# globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\n",
    "# evaluate_wfd_words(globals()[this+\"_sample\"], voiced = (\"b\", \"d\", \"g\"))\n",
    "# print(\"--------------------------\")\n",
    "# print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "# print(\"--------------------------\")\n",
    "# print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "# print(\"--------------------------\")\n",
    "# print(\"Grammars:\", globals()[this].grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### German word-final devoicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this = \"mtsl3\"\n",
    "# globals()[this] = MTSL(polar = \"n\")\n",
    "# globals()[this].data = german_wfd\n",
    "# globals()[this].data.append(\"\") # added to eliminate *>< on all tiers\n",
    "# globals()[this].extract_alphabet()\n",
    "# globals()[this].learn()\n",
    "# globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\n",
    "# evaluate_wfd_words(globals()[this+\"_sample\"], voiced = (\"b\", \"d\", \"g\"))\n",
    "# print(\"--------------------------\")\n",
    "# print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "# print(\"--------------------------\")\n",
    "# print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "# print(\"--------------------------\")\n",
    "# print(\"Grammars:\", globals()[this].grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Single vowel harmony, no blockers\n",
    "\n",
    "### Artificial grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of harmonic words: 100%.\n",
      "--------------------------\n",
      "Generates such strings: ['', 'ox', 'a', 'a', '', 'xx', 'ox', 'oo', 'axxa', 'oxoo', 'aaa', 'oxooox', '', 'ox', 'aax']\n",
      "--------------------------\n",
      "Size of the grammar: 1\n",
      "--------------------------\n",
      "Grammars: {('a', 'o'): [('o', 'a'), ('a', 'o')]}\n"
     ]
    }
   ],
   "source": [
    "this = \"mtsl4\"\n",
    "globals()[this] = MTSL(polar = \"n\")\n",
    "globals()[this].data = toy_vhnb\n",
    "globals()[this].data.append(\"\") # added to eliminate *>< on all tiers\n",
    "globals()[this].extract_alphabet()\n",
    "globals()[this].learn()\n",
    "globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\n",
    "harmonic_evaluator(globals()[this+\"_sample\"], single_harmony_no_blockers)\n",
    "print(\"--------------------------\")\n",
    "print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "print(\"--------------------------\")\n",
    "print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "print(\"--------------------------\")\n",
    "print(\"Grammars:\", globals()[this].grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplified Finnish harmony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this = \"mtsl5\"\n",
    "# globals()[this] = MTSL(polar = \"n\")\n",
    "# globals()[this].data = finnish_harmony_masked\n",
    "# globals()[this].data.append(\"\") # added to eliminate *>< on all tiers\n",
    "# globals()[this].extract_alphabet()\n",
    "# globals()[this].learn()\n",
    "# globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\n",
    "# harmonic_evaluator(globals()[this+\"_sample\"], front_harmony)\n",
    "# print(\"--------------------------\")\n",
    "# print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "# print(\"--------------------------\")\n",
    "# print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "# print(\"--------------------------\")\n",
    "# print(\"Grammars:\", globals()[this].grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finnish harmony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this = \"mtsl6\"\n",
    "# globals()[this] = MTSL(polar = \"n\")\n",
    "# globals()[this].data = finnish_harmony\n",
    "# globals()[this].data.append(\"\") # added to eliminate *>< on all tiers\n",
    "# globals()[this].extract_alphabet()\n",
    "# globals()[this].learn()\n",
    "# globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\n",
    "# harmonic_evaluator(globals()[this+\"_sample\"], front_harmony)\n",
    "# print(\"--------------------------\")\n",
    "# print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "# print(\"--------------------------\")\n",
    "# print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "# print(\"--------------------------\")\n",
    "# print(\"Grammars:\", globals()[this].grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: Single vowel harmony with blockers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of harmonic words: 100%.\n",
      "--------------------------\n",
      "Generates such strings: ['oo', 'xaafxa', 'x', 'o', '', 'xxofxaxffa', 'oofx', 'oxffff', 'xfxxxaf', 'xxoo', '', 'aaaf', '', 'a', '']\n",
      "--------------------------\n",
      "Size of the grammar: 3\n",
      "--------------------------\n",
      "Grammars: {('a', 'f', 'o'): [('o', 'a')], ('f', 'o'): [('f', 'o')], ('a', 'o'): [('a', 'o')]}\n"
     ]
    }
   ],
   "source": [
    "this = \"mtsl7\"\n",
    "globals()[this] = MTSL(polar = \"n\")\n",
    "globals()[this].data = toy_vhwb\n",
    "globals()[this].data.append(\"\") # added to eliminate *>< on all tiers\n",
    "globals()[this].extract_alphabet()\n",
    "globals()[this].learn()\n",
    "globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\n",
    "harmonic_evaluator(globals()[this+\"_sample\"], single_harmony_with_blockers)\n",
    "print(\"--------------------------\")\n",
    "print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "print(\"--------------------------\")\n",
    "print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "print(\"--------------------------\")\n",
    "print(\"Grammars:\", globals()[this].grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 4: Two vowel harmonies, no blockers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of harmonic words: 100%.\n",
      "--------------------------\n",
      "Generates such strings: ['xa', 'xaa', 'a', 'uuuxxxuxuuxxxux', 'x', 'oxo', 'o', 'exex', 'oox', 'uuuxx', 'o', '', 'eee', 'uxxuxx', '']\n",
      "--------------------------\n",
      "Size of the grammar: 6\n",
      "--------------------------\n",
      "Grammars: {('a', 'u'): [('a', 'u'), ('u', 'a')], ('e', 'o'): [('o', 'e'), ('e', 'o')], ('e', 'u'): [('u', 'e'), ('e', 'u')], ('o', 'u'): [('u', 'o'), ('o', 'u')], ('a', 'o'): [('a', 'o'), ('o', 'a')], ('a', 'e'): [('e', 'a'), ('a', 'e')]}\n"
     ]
    }
   ],
   "source": [
    "this = \"mtsl8\"\n",
    "globals()[this] = MTSL(polar = \"n\")\n",
    "globals()[this].data = toy_shnb\n",
    "globals()[this].data.append(\"\") # added to eliminate *>< on all tiers\n",
    "globals()[this].extract_alphabet()\n",
    "globals()[this].learn()\n",
    "globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\n",
    "harmonic_evaluator(globals()[this+\"_sample\"], double_harmony)\n",
    "print(\"--------------------------\")\n",
    "print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "print(\"--------------------------\")\n",
    "print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "print(\"--------------------------\")\n",
    "print(\"Grammars:\", globals()[this].grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 5: Two vowel harmonies with vowel blockers\n",
    "\n",
    "### Artificial grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of harmonic words: 98%.\n",
      "--------------------------\n",
      "Generates such strings: ['', 'oa', 'IIaI', 'axIIaxIxxIxaaaaI', 'IaxxxI', 'I', '', 'IxxIIIax', 'Oxie', 'o', 'aIxIxaxI', 'uxaxxxIxx', 'xxi', 'a', 'e']\n",
      "--------------------------\n",
      "Size of the grammar: 32\n",
      "--------------------------\n",
      "Grammars: {('o',): [('o', 'o')], ('e', 'o'): [('o', 'e'), ('e', 'o')], ('i', 'u'): [('u', 'i'), ('i', 'u')], ('U', 'i'): [('i', 'U')], ('O', 'U', 'e', 'i', 'x'): [('O', 'i')], ('I', 'U'): [('U', 'I'), ('I', 'U')], ('a', 'e'): [('e', 'a'), ('a', 'e')], ('a', 'i'): [('a', 'i'), ('i', 'a')], ('O', 'a'): [('a', 'O'), ('O', 'a')], ('O', 'u'): [('u', 'O'), ('O', 'u')], ('I', 'a', 'u'): [('u', 'I')], ('O', 'o'): [('o', 'O'), ('O', 'o')], ('I', 'u'): [('I', 'u')], ('O', 'U'): [('U', 'O')], ('U', 'a'): [('U', 'a'), ('a', 'U')], ('I', 'e'): [('I', 'e'), ('e', 'I')], ('I', 'o'): [('I', 'o')], ('i', 'o'): [('i', 'o'), ('o', 'i')], ('O',): [('O', 'O')], ('O', 'e'): [('e', 'O')], ('I', 'O'): [('I', 'O'), ('O', 'I')], ('I', 'a', 'o'): [('o', 'I')], ('I', 'i'): [('i', 'I'), ('I', 'i')], ('U', 'o'): [('o', 'U'), ('U', 'o')], ('a', 'u'): [('a', 'u')], ('U', 'u'): [('u', 'U'), ('U', 'u')], ('U', 'e'): [('e', 'U')], ('e', 'u'): [('u', 'e'), ('e', 'u')], ('O', 'i'): [('i', 'O')], ('o', 'u'): [('u', 'o')], ('a', 'o'): [('a', 'o')], ('U', 'e', 'i', 'x'): [('U', 'i')]}\n"
     ]
    }
   ],
   "source": [
    "this = \"mtsl9\"\n",
    "globals()[this] = MTSL(polar = \"n\")\n",
    "globals()[this].data = toy_mhwb\n",
    "globals()[this].data.append(\"\") # added to eliminate *>< on all tiers\n",
    "globals()[this].extract_alphabet()\n",
    "globals()[this].learn()\n",
    "globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\n",
    "harmonic_evaluator(globals()[this+\"_sample\"], backness_and_rounding)\n",
    "print(\"--------------------------\")\n",
    "print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "print(\"--------------------------\")\n",
    "print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "print(\"--------------------------\")\n",
    "print(\"Grammars:\", globals()[this].grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplified Turkish harmony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this = \"mtsl10\"\n",
    "# globals()[this] = MTSL(polar = \"n\")\n",
    "# globals()[this].data = turkish_harmony_masked\n",
    "# globals()[this].data.append(\"\") # added to eliminate *>< on all tiers\n",
    "# globals()[this].extract_alphabet()\n",
    "# globals()[this].learn()\n",
    "# globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\n",
    "# harmonic_evaluator(globals()[this+\"_sample\"], backness_and_rounding)\n",
    "# print(\"--------------------------\")\n",
    "# print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "# print(\"--------------------------\")\n",
    "# print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "# print(\"--------------------------\")\n",
    "# print(\"Grammars:\", globals()[this].grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turkish harmony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this = \"mtsl11\"\n",
    "# globals()[this] = MTSL(polar = \"n\")\n",
    "# globals()[this].data = turkish_harmony\n",
    "# globals()[this].data.append(\"\") # added to eliminate *>< on all tiers\n",
    "# globals()[this].extract_alphabet()\n",
    "# globals()[this].learn()\n",
    "# globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\n",
    "# harmonic_evaluator(globals()[this+\"_sample\"], backness_and_rounding)\n",
    "# print(\"--------------------------\")\n",
    "# print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "# print(\"--------------------------\")\n",
    "# print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "# print(\"--------------------------\")\n",
    "# print(\"Grammars:\", globals()[this].grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 6: Vowel harmony and consonant harmony, no blockers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of harmonic words: 100%.\n",
      "--------------------------\n",
      "Generates such strings: ['o', 'ap', 'bbo', 'apaa', 'p', 'bbab', 'o', 'o', 'bo', 'op', '', '', 'po', 'apa', 'apapp']\n",
      "--------------------------\n",
      "Size of the grammar: 2\n",
      "--------------------------\n",
      "Grammars: {('a', 'o'): [('o', 'a'), ('a', 'o')], ('b', 'p'): [('b', 'p'), ('p', 'b')]}\n"
     ]
    }
   ],
   "source": [
    "this = \"mtsl12\"\n",
    "globals()[this] = MTSL(polar = \"n\")\n",
    "globals()[this].data = toy_dhnb\n",
    "globals()[this].data.append(\"\") # added to eliminate *>< on all tiers\n",
    "globals()[this].extract_alphabet()\n",
    "globals()[this].learn()\n",
    "globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\n",
    "harmonic_evaluator(globals()[this+\"_sample\"], double_harmony_no_blockers)\n",
    "print(\"--------------------------\")\n",
    "print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "print(\"--------------------------\")\n",
    "print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "print(\"--------------------------\")\n",
    "print(\"Grammars:\", globals()[this].grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 7: Vowel harmony and consonant harmony with blockers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of harmonic words: 100%.\n",
      "--------------------------\n",
      "Generates such strings: ['topttp', 'aapt', 'b', 'oot', 'topot', 'tpotpt', 'op', 'o', 'otppppoppp', 'tooottp', 'b', '', 'a', 'bbto', 'poopo']\n",
      "--------------------------\n",
      "Size of the grammar: 4\n",
      "--------------------------\n",
      "Grammars: {('b', 'p'): [('p', 'b')], ('b', 'p', 't'): [('b', 'p')], ('a', 'o'): [('a', 'o'), ('o', 'a')], ('b', 't'): [('t', 'b')]}\n"
     ]
    }
   ],
   "source": [
    "this = \"mtsl13\"\n",
    "globals()[this] = MTSL(polar = \"n\")\n",
    "globals()[this].data = toy_dhwb\n",
    "globals()[this].data.append(\"\") # added to eliminate *>< on all tiers\n",
    "globals()[this].extract_alphabet()\n",
    "globals()[this].learn()\n",
    "globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\n",
    "harmonic_evaluator(globals()[this+\"_sample\"], double_harmony_with_blockers)\n",
    "print(\"--------------------------\")\n",
    "print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "print(\"--------------------------\")\n",
    "print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "print(\"--------------------------\")\n",
    "print(\"Grammars:\", globals()[this].grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 8: Unbounded tonal plateauing\n",
    "\n",
    "Impossible to check, we cannot have $3$-local MTSL learner. (But of course it'll fail.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 9: First-last harmony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of first-last harmonic words: 50%.\n",
      "--------------------------\n",
      "Generates such strings: ['a', '', 'a', 'oxo', 'aoxoo', 'axaaoxooaooxaoxo', '', 'ooxo', 'a', '', 'oa', '', 'o', 'a', '']\n",
      "--------------------------\n",
      "Size of the grammar: 1\n",
      "--------------------------\n",
      "Grammars: {('a', 'o', 'x'): [('>', 'x'), ('x', '<')]}\n"
     ]
    }
   ],
   "source": [
    "this = \"mtsl15\"\n",
    "globals()[this] = MTSL(polar = \"n\")\n",
    "globals()[this].data = first_last_data\n",
    "globals()[this].data.append(\"\") # added to eliminate *>< on all tiers\n",
    "globals()[this].extract_alphabet()\n",
    "globals()[this].learn()\n",
    "globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\n",
    "evaluate_first_last_words(globals()[this+\"_sample\"])\n",
    "print(\"--------------------------\")\n",
    "print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "print(\"--------------------------\")\n",
    "print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "print(\"--------------------------\")\n",
    "print(\"Grammars:\", globals()[this].grammar)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
